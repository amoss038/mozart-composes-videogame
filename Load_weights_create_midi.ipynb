{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /Users/amossworkcomp/opt/anaconda3/lib/python3.7/site-packages (2.4.3)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /Users/amossworkcomp/opt/anaconda3/lib/python3.7/site-packages (from keras) (1.19.5)\n",
      "Requirement already satisfied: h5py in /Users/amossworkcomp/opt/anaconda3/lib/python3.7/site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /Users/amossworkcomp/opt/anaconda3/lib/python3.7/site-packages (from keras) (1.5.0)\n",
      "Requirement already satisfied: pyyaml in /Users/amossworkcomp/opt/anaconda3/lib/python3.7/site-packages (from keras) (5.3.1)\n",
      "Requirement already satisfied: six in /Users/amossworkcomp/opt/anaconda3/lib/python3.7/site-packages (from h5py->keras) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.3'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import keras\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "from music21 import converter, instrument, stream, note, chord\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, Activation, Bidirectional, Flatten\n",
    "from keras import utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "from keras.layers import BatchNormalization as BatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file = open(\"data/notes\", \"rb\")\n",
    "notes = pickle.load(pickle_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB = (len(set(notes)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(lstm_input):\n",
    "    '''\n",
    "    Build and compile the model\n",
    "    \n",
    "    lstm_input: lstm_input.shape[1] = number of steps, lstm_input.shape[2] = number features needed for\n",
    "    Bi-directional\n",
    "    \n",
    "    model_output: number of categories to output for classification\n",
    "    \n",
    "    returns: compiled model\n",
    "    '''\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(512,input_shape=(lstm_input.shape[1], lstm_input.shape[2]), return_sequences=True)))\n",
    "    model.add(SeqSelfAttention(attention_activation='sigmoid'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(BatchNorm())\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(512,input_shape=(lstm_input.shape[1], lstm_input.shape[2]), return_sequences=True)))\n",
    "    model.add(SeqSelfAttention(attention_activation='sigmoid'))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(512,input_shape=(lstm_input.shape[1], lstm_input.shape[2]), return_sequences=False)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(BatchNorm())\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(VOCAB))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "    \n",
    "    model.load_weights('models_weights/weights_attention.h5')\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_notes(notes, pitches, VOCAB):\n",
    "    '''\n",
    "    prepare lstm input notes again used by the network to predict notes\n",
    "    \n",
    "    '''\n",
    "    #setting the sequence length to 100\n",
    "    #print(len(set(notes)))\n",
    "    sequence = 100 \n",
    "    \n",
    "    #creating the note to int dict to map pitches to integers\n",
    "    note_dict = dict((note, number) for number, note in enumerate(pitches))\n",
    "    #print(note_dict)\n",
    "    lstm_input = []\n",
    "    lstm_output = []\n",
    "    \n",
    "    #creating inputs and corresponding outputs\n",
    "    for i in range(0, len(notes)- sequence, 1):\n",
    "        inputs = notes[i : i + sequence]\n",
    "        outputs = notes[i + sequence]\n",
    "        lstm_input.append([note_dict[pitch] for pitch in inputs])\n",
    "        lstm_output.append(note_dict[outputs])\n",
    "    \n",
    "    #creating all the objects to reshape network input to make compatable with lstm network\n",
    "    shape_1 = lstm_input\n",
    "    shape_2 = len(lstm_input)\n",
    "    shape_3 = sequence \n",
    "    \n",
    "    #reshaping lstm input for lstm\n",
    "    lstm_input = np.reshape(shape_1, (shape_2, shape_3, 1))\n",
    "    \n",
    "    #normalize lstm input with  number of unique notes\n",
    "    lstm_normalized = lstm_input / float(len(pitches))\n",
    "    \n",
    "    return lstm_input, lstm_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n",
      "1327\n"
     ]
    }
   ],
   "source": [
    "def predict_to_notes(model, lstm_input, pitches, VOCAB):\n",
    "    '''\n",
    "    **Generated predictions from model based on random starting point**\n",
    "    \n",
    "    model: --> Original model with weights loaded\n",
    "    lstm_inputL: --> Output from prepare, used to initialize pattern with an int the model recognizes\n",
    "    pitches: ---> list of all the notes, chords and rests\n",
    "    VOCAB:----> Number of unique notes to classify = len(pitches)\n",
    "    \n",
    "    output: Predicted_notes \n",
    "    '''\n",
    "    #random starting point \n",
    "    start = np.random.randint(0, len(lstm_input) -1)\n",
    "    note_dict = dict((number, note) for number, note in enumerate(pitches))\n",
    "    \n",
    "    pattern = lstm_input[start]\n",
    "   \n",
    "    predicted_notes = []\n",
    "    \n",
    "    \n",
    "    for note in range(500):\n",
    "        to_predict = np.reshape(pattern, (1, len(pattern), 1))\n",
    "        to_predict = to_predict/ float(VOCAB)\n",
    "        #print(to_predict.shape)\n",
    "        \n",
    "        \n",
    "        prediction = model.predict(to_predict, verbose = 0)\n",
    "        \n",
    "        \n",
    "        index = np.argmax(prediction)\n",
    "        print(index)\n",
    "        result = note_dict[index]\n",
    "        predicted_notes.append(result)\n",
    "        \n",
    "        np.append(pattern, index)\n",
    "        pattern = pattern[0:len(pattern)]\n",
    "        \n",
    "        \n",
    "    return predicted_notes\n",
    "predicted_notes = predict_to_notes(model, lstm_input, pitches, VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def midi_convert(predicted_notes):\n",
    "    '''\n",
    "    convert the notes in predicted_notes to midi files\n",
    "    \n",
    "    predicted_notes: Output from function predict_to_notes() --> list of predicted notes\n",
    "    \n",
    "    returns: None --- > Creates a midi file when ran\n",
    "    '''\n",
    "    offset = 0 \n",
    "    midi_notes = []\n",
    "    \n",
    "    #create notes, chords, and rest objects from predicted_notes\n",
    "    for pattern in predicted_notes:\n",
    "        pattern = pattern.split()\n",
    "        temp = pattern[0]\n",
    "        duration = pattern[1]\n",
    "        pattern = temp\n",
    "        #checking to see if a note is a chord \n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in chord:\n",
    "                this_note = note.Note(int(current_note))\n",
    "                this_note.storedInstrument = instrument.Piano()\n",
    "                notes.append(this_note)\n",
    "            new_chord = chord.Chord(notes) \n",
    "            new_chord.offset = offset\n",
    "            midi_notes.append(new_chord)\n",
    "        #if the pattern is a rest    \n",
    "        elif ('rest' in pattern):\n",
    "            this_rest = note.Rest(pattern)\n",
    "            this_rest.offset = offset\n",
    "            this_rest.storedInstrument = instrument.Piano() #Still needs to be paino instrument even though = rest\n",
    "            midi_notes.append(this_rest)\n",
    "        else:\n",
    "            this_note = note.Note(pattern)\n",
    "            this_note.offset = offset \n",
    "            this_note.storedInstrument = instrument.Piano()\n",
    "            midi_notes.append(this_note)\n",
    "        #ensure that the notes do not stack    \n",
    "        offset += convert_to_float(duration)\n",
    "    \n",
    "    midi = stream.Stream(midi_notes)\n",
    "    midi.write('midi', fp = 'predicted_song.mid')\n",
    "    \n",
    "    \n",
    "#From: https://stackoverflow.com/questions/1806278/convert-fraction-to-float\n",
    "def convert_to_float(frac_str):\n",
    "    try:\n",
    "        return float(frac_str)\n",
    "    except ValueError:\n",
    "        num, denom = frac_str.split('/')\n",
    "        try:\n",
    "            leading, num = num.split(' ')\n",
    "            whole = float(leading)\n",
    "        except ValueError:\n",
    "            whole = 0\n",
    "        frac = float(num) / float(denom)\n",
    "        return whole - frac if whole < 0 else whole + frac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate():\n",
    "    '''\n",
    "    generates the midi file\n",
    "    \n",
    "    \n",
    "    \n",
    "    output:None ---> creates the midi file\n",
    "    \n",
    "    '''\n",
    "    #Load the notes\n",
    "    pickle_file = open(\"data/notes\", \"rb\")\n",
    "    notes = pickle.load(pickle_file)\n",
    "    \n",
    "    #create the pitchnames\n",
    "    pitches = sorted(set(note for note in notes))\n",
    "    \n",
    "    \n",
    "    lstm_input, lstm_normalized = prepare_notes(notes, pitches, VOCAB)\n",
    "    model = build_model(lstm_normalized)\n",
    "    model.load_weights('models_weights/weights_attention.h5.h5')\n",
    "    predicted_notes = predict_to_notes(model, lstm_input, pitches, VOCAB)\n",
    "    midi_convert(predicted_notes)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to load weights saved in HDF5 format into a subclassed Model which has not created its variables yet. Call the Model first, then load the weights.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-b97e0091fa1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-31-9aaff5f5e8b1>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mlstm_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_notes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnotes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpitches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVOCAB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_normalized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'models_weights/weights_attention.h5.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mpredicted_notes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_to_notes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpitches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVOCAB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-19acceed2cab>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(lstm_input)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rmsprop'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'models_weights/weights_attention.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[1;32m   2221\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2222\u001b[0m       raise ValueError(\n\u001b[0;32m-> 2223\u001b[0;31m           \u001b[0;34m'Unable to load weights saved in HDF5 format into a subclassed '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2224\u001b[0m           \u001b[0;34m'Model which has not created its variables yet. Call the Model '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2225\u001b[0m           'first, then load the weights.')\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to load weights saved in HDF5 format into a subclassed Model which has not created its variables yet. Call the Model first, then load the weights."
     ]
    }
   ],
   "source": [
    "generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('first_train.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('models_weights/first_try_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_2 (Bidirection (None, 100, 1024)         2105344   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100, 1024)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 102400)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2424)              248220024 \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2424)              0         \n",
      "=================================================================\n",
      "Total params: 250,325,368\n",
      "Trainable params: 250,325,368\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = tf.keras.models.load_model('models_weights/attention_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras_self_attention import SeqSelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
