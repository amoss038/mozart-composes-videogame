{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_att_no_time = '''Epoch 1/130\n",
    "571/571 [==============================] - 140s 227ms/step - loss: 5.8184\n",
    "\n",
    "Epoch 00001: loss improved from inf to 5.66126, saving model to bb_att_only.hdf5\n",
    "Epoch 2/130\n",
    "571/571 [==============================] - 130s 228ms/step - loss: 5.5119\n",
    "\n",
    "Epoch 00002: loss improved from 5.66126 to 5.51396, saving model to bb_att_only.hdf5\n",
    "Epoch 3/130\n",
    "571/571 [==============================] - 130s 228ms/step - loss: 5.4715\n",
    "\n",
    "Epoch 00003: loss improved from 5.51396 to 5.46139, saving model to bb_att_only.hdf5\n",
    "Epoch 4/130\n",
    "571/571 [==============================] - 130s 227ms/step - loss: 5.3565\n",
    "\n",
    "Epoch 00004: loss improved from 5.46139 to 5.37422, saving model to bb_att_only.hdf5\n",
    "Epoch 5/130\n",
    "571/571 [==============================] - 130s 228ms/step - loss: 5.2860\n",
    "\n",
    "Epoch 00005: loss improved from 5.37422 to 5.29255, saving model to bb_att_only.hdf5\n",
    "Epoch 6/130\n",
    "571/571 [==============================] - 130s 228ms/step - loss: 5.2239\n",
    "\n",
    "Epoch 00006: loss improved from 5.29255 to 5.21631, saving model to bb_att_only.hdf5\n",
    "Epoch 7/130\n",
    "571/571 [==============================] - 130s 228ms/step - loss: 5.1509\n",
    "\n",
    "Epoch 00007: loss improved from 5.21631 to 5.13898, saving model to bb_att_only.hdf5\n",
    "Epoch 8/130\n",
    "571/571 [==============================] - 130s 228ms/step - loss: 5.0591\n",
    "\n",
    "Epoch 00008: loss improved from 5.13898 to 5.06011, saving model to bb_att_only.hdf5\n",
    "Epoch 9/130\n",
    "571/571 [==============================] - 130s 227ms/step - loss: 4.9468\n",
    "\n",
    "Epoch 00009: loss improved from 5.06011 to 4.95275, saving model to bb_att_only.hdf5\n",
    "Epoch 10/130\n",
    "571/571 [==============================] - 129s 227ms/step - loss: 4.8351\n",
    "\n",
    "Epoch 00010: loss improved from 4.95275 to 4.83982, saving model to bb_att_only.hdf5\n",
    "Epoch 11/130\n",
    "571/571 [==============================] - 130s 227ms/step - loss: 4.7021\n",
    "\n",
    "Epoch 00011: loss improved from 4.83982 to 4.70484, saving model to bb_att_only.hdf5\n",
    "Epoch 12/130\n",
    "571/571 [==============================] - 130s 228ms/step - loss: 4.5737\n",
    "\n",
    "Epoch 00012: loss improved from 4.70484 to 4.56314, saving model to bb_att_only.hdf5\n",
    "Epoch 13/130\n",
    "571/571 [==============================] - 130s 227ms/step - loss: 4.4303\n",
    "\n",
    "Epoch 00013: loss improved from 4.56314 to 4.42760, saving model to bb_att_only.hdf5\n",
    "Epoch 14/130\n",
    "571/571 [==============================] - 130s 227ms/step - loss: 4.2789\n",
    "\n",
    "Epoch 00014: loss improved from 4.42760 to 4.30009, saving model to bb_att_only.hdf5\n",
    "Epoch 15/130\n",
    "571/571 [==============================] - 130s 228ms/step - loss: 4.1387\n",
    "\n",
    "Epoch 00015: loss improved from 4.30009 to 4.17526, saving model to bb_att_only.hdf5\n",
    "Epoch 16/130\n",
    "571/571 [==============================] - 130s 228ms/step - loss: 4.0441\n",
    "\n",
    "Epoch 00016: loss improved from 4.17526 to 4.05342, saving model to bb_att_only.hdf5\n",
    "Epoch 17/130\n",
    "571/571 [==============================] - 130s 228ms/step - loss: 3.9120\n",
    "\n",
    "Epoch 00017: loss improved from 4.05342 to 3.93348, saving model to bb_att_only.hdf5\n",
    "Epoch 18/130\n",
    "571/571 [==============================] - 130s 227ms/step - loss: 3.7917\n",
    "\n",
    "Epoch 00018: loss improved from 3.93348 to 3.81204, saving model to bb_att_only.hdf5\n",
    "Epoch 19/130\n",
    "571/571 [==============================] - 130s 227ms/step - loss: 3.6618\n",
    "\n",
    "Epoch 00019: loss improved from 3.81204 to 3.69756, saving model to bb_att_only.hdf5\n",
    "Epoch 20/130\n",
    "571/571 [==============================] - 129s 227ms/step - loss: 3.5424\n",
    "\n",
    "Epoch 00020: loss improved from 3.69756 to 3.58150, saving model to bb_att_only.hdf5\n",
    "Epoch 21/130\n",
    "571/571 [==============================] - 129s 227ms/step - loss: 3.4268\n",
    "\n",
    "Epoch 00021: loss improved from 3.58150 to 3.45269, saving model to bb_att_only.hdf5\n",
    "Epoch 22/130\n",
    "571/571 [==============================] - 129s 227ms/step - loss: 3.2990\n",
    "\n",
    "Epoch 00022: loss improved from 3.45269 to 3.33928, saving model to bb_att_only.hdf5\n",
    "Epoch 23/130\n",
    "571/571 [==============================] - 129s 227ms/step - loss: 3.1826\n",
    "\n",
    "Epoch 00023: loss improved from 3.33928 to 3.22760, saving model to bb_att_only.hdf5\n",
    "Epoch 24/130\n",
    "571/571 [==============================] - 129s 226ms/step - loss: 3.0857\n",
    "\n",
    "Epoch 00024: loss improved from 3.22760 to 3.13269, saving model to bb_att_only.hdf5\n",
    "Epoch 25/130\n",
    "571/571 [==============================] - 129s 226ms/step - loss: 3.0171\n",
    "\n",
    "Epoch 00025: loss improved from 3.13269 to 3.05219, saving model to bb_att_only.hdf5\n",
    "Epoch 26/130\n",
    "571/571 [==============================] - 129s 227ms/step - loss: 2.9120\n",
    "\n",
    "Epoch 00026: loss improved from 3.05219 to 2.94800, saving model to bb_att_only.hdf5\n",
    "Epoch 27/130\n",
    "571/571 [==============================] - 129s 227ms/step - loss: 2.7830\n",
    "\n",
    "Epoch 00027: loss improved from 2.94800 to 2.84036, saving model to bb_att_only.hdf5\n",
    "Epoch 28/130\n",
    "571/571 [==============================] - 129s 226ms/step - loss: 2.6547\n",
    "\n",
    "Epoch 00028: loss improved from 2.84036 to 2.72866, saving model to bb_att_only.hdf5\n",
    "Epoch 29/130\n",
    "571/571 [==============================] - 130s 227ms/step - loss: 2.5868\n",
    "\n",
    "Epoch 00029: loss improved from 2.72866 to 2.64827, saving model to bb_att_only.hdf5\n",
    "Epoch 30/130\n",
    "571/571 [==============================] - 129s 227ms/step - loss: 2.4929\n",
    "\n",
    "Epoch 00030: loss improved from 2.64827 to 2.56782, saving model to bb_att_only.hdf5\n",
    "Epoch 31/130\n",
    "571/571 [==============================] - 130s 227ms/step - loss: 2.4483\n",
    "\n",
    "Epoch 00031: loss improved from 2.56782 to 2.50748, saving model to bb_att_only.hdf5\n",
    "Epoch 32/130\n",
    "571/571 [==============================] - 130s 228ms/step - loss: 2.3724\n",
    "\n",
    "Epoch 00032: loss improved from 2.50748 to 2.42940, saving model to bb_att_only.hdf5\n",
    "Epoch 33/130\n",
    "571/571 [==============================] - 130s 228ms/step - loss: 2.3008\n",
    "\n",
    "Epoch 00033: loss improved from 2.42940 to 2.34655, saving model to bb_att_only.hdf5\n",
    "Epoch 34/130\n",
    "571/571 [==============================] - 130s 228ms/step - loss: 2.2159\n",
    "\n",
    "Epoch 00034: loss improved from 2.34655 to 2.28011, saving model to bb_att_only.hdf5\n",
    "Epoch 35/130\n",
    "571/571 [==============================] - 130s 228ms/step - loss: 2.1273\n",
    "\n",
    "Epoch 00035: loss improved from 2.28011 to 2.19951, saving model to bb_att_only.hdf5\n",
    "Epoch 36/130\n",
    "571/571 [==============================] - 130s 228ms/step - loss: 2.0932\n",
    "\n",
    "Epoch 00036: loss improved from 2.19951 to 2.15080, saving model to bb_att_only.hdf5\n",
    "Epoch 37/130\n",
    "571/571 [==============================] - 130s 228ms/step - loss: 2.0341\n",
    "\n",
    "Epoch 00037: loss improved from 2.15080 to 2.08680, saving model to bb_att_only.hdf5\n",
    "Epoch 38/130\n",
    "571/571 [==============================] - 130s 228ms/step - loss: 1.9729\n",
    "\n",
    "Epoch 00038: loss improved from 2.08680 to 2.02544, saving model to bb_att_only.hdf5\n",
    "Epoch 39/130\n",
    "571/571 [==============================] - 130s 227ms/step - loss: 1.9208\n",
    "\n",
    "Epoch 00039: loss improved from 2.02544 to 1.98125, saving model to bb_att_only.hdf5\n",
    "Epoch 40/130\n",
    "571/571 [==============================] - 130s 228ms/step - loss: 1.8710\n",
    "\n",
    "Epoch 00040: loss improved from 1.98125 to 1.94102, saving model to bb_att_only.hdf5\n",
    "Epoch 41/130\n",
    "571/571 [==============================] - 130s 227ms/step - loss: 1.8212\n",
    "\n",
    "Epoch 00041: loss improved from 1.94102 to 1.88716, saving model to bb_att_only.hdf5\n",
    "Epoch 42/130\n",
    "571/571 [==============================] - 130s 228ms/step - loss: 1.7568\n",
    "\n",
    "Epoch 00042: loss improved from 1.88716 to 1.83377, saving model to bb_att_only.hdf5\n",
    "Epoch 43/130\n",
    "571/571 [==============================] - 130s 227ms/step - loss: 1.7211\n",
    "\n",
    "Epoch 00043: loss improved from 1.83377 to 1.79191, saving model to bb_att_only.hdf5\n",
    "Epoch 44/130\n",
    "571/571 [==============================] - 130s 227ms/step - loss: 1.6731\n",
    "\n",
    "Epoch 00044: loss improved from 1.79191 to 1.75529, saving model to bb_att_only.hdf5\n",
    "Epoch 45/130\n",
    "571/571 [==============================] - 130s 228ms/step - loss: 1.6573\n",
    "\n",
    "Epoch 00045: loss improved from 1.75529 to 1.72155, saving model to bb_att_only.hdf5\n",
    "Epoch 46/130\n",
    "571/571 [==============================] - 131s 229ms/step - loss: 1.6183\n",
    "\n",
    "Epoch 00046: loss improved from 1.72155 to 1.66719, saving model to bb_att_only.hdf5\n",
    "Epoch 47/130\n",
    "571/571 [==============================] - 131s 229ms/step - loss: 1.5764\n",
    "\n",
    "Epoch 00047: loss improved from 1.66719 to 1.63465, saving model to bb_att_only.hdf5\n",
    "Epoch 48/130\n",
    "571/571 [==============================] - 131s 229ms/step - loss: 1.5154\n",
    "\n",
    "Epoch 00048: loss improved from 1.63465 to 1.58724, saving model to bb_att_only.hdf5\n",
    "Epoch 49/130\n",
    "571/571 [==============================] - 130s 228ms/step - loss: 1.4884\n",
    "\n",
    "Epoch 00049: loss improved from 1.58724 to 1.54491, saving model to bb_att_only.hdf5\n",
    "Epoch 50/130\n",
    "571/571 [==============================] - 130s 228ms/step - loss: 1.4380\n",
    "\n",
    "Epoch 00050: loss improved from 1.54491 to 1.50432, saving model to bb_att_only.hdf5\n",
    "Epoch 51/130\n",
    "571/571 [==============================] - 130s 228ms/step - loss: 1.4147\n",
    "\n",
    "Epoch 00051: loss improved from 1.50432 to 1.47420, saving model to bb_att_only.hdf5\n",
    "Epoch 52/130\n",
    "571/571 [==============================] - 130s 227ms/step - loss: 1.3780\n",
    "\n",
    "Epoch 00052: loss improved from 1.47420 to 1.45008, saving model to bb_att_only.hdf5\n",
    "Epoch 53/130\n",
    "571/571 [==============================] - 129s 227ms/step - loss: 1.3597\n",
    "\n",
    "Epoch 00053: loss improved from 1.45008 to 1.40026, saving model to bb_att_only.hdf5\n",
    "Epoch 54/130\n",
    "571/571 [==============================] - 130s 227ms/step - loss: 1.3413\n",
    "\n",
    "Epoch 00054: loss improved from 1.40026 to 1.38786, saving model to bb_att_only.hdf5\n",
    "Epoch 55/130\n",
    "571/571 [==============================] - 129s 227ms/step - loss: 1.3144\n",
    "\n",
    "Epoch 00055: loss improved from 1.38786 to 1.35834, saving model to bb_att_only.hdf5\n",
    "Epoch 56/130\n",
    "571/571 [==============================] - 129s 227ms/step - loss: 1.2576\n",
    "\n",
    "Epoch 00056: loss improved from 1.35834 to 1.32077, saving model to bb_att_only.hdf5\n",
    "Epoch 57/130\n",
    "571/571 [==============================] - 130s 227ms/step - loss: 1.2557\n",
    "\n",
    "Epoch 00057: loss improved from 1.32077 to 1.29260, saving model to bb_att_only.hdf5\n",
    "Epoch 58/130\n",
    "571/571 [==============================] - 130s 227ms/step - loss: 1.1956\n",
    "\n",
    "Epoch 00058: loss improved from 1.29260 to 1.26383, saving model to bb_att_only.hdf5\n",
    "Epoch 59/130\n",
    "571/571 [==============================] - 130s 227ms/step - loss: 1.1875\n",
    "\n",
    "Epoch 00059: loss improved from 1.26383 to 1.22620, saving model to bb_att_only.hdf5\n",
    "Epoch 60/130\n",
    "571/571 [==============================] - 130s 228ms/step - loss: 1.1536\n",
    "\n",
    "Epoch 00060: loss improved from 1.22620 to 1.20709, saving model to bb_att_only.hdf5\n",
    "Epoch 61/130\n",
    "571/571 [==============================] - 130s 227ms/step - loss: 1.1428\n",
    "\n",
    "Epoch 00061: loss improved from 1.20709 to 1.18088, saving model to bb_att_only.hdf5\n",
    "Epoch 62/130\n",
    "571/571 [==============================] - 130s 228ms/step - loss: 1.1154\n",
    "\n",
    "Epoch 00062: loss improved from 1.18088 to 1.15217, saving model to bb_att_only.hdf5\n",
    "Epoch 63/130\n",
    "571/571 [==============================] - 130s 228ms/step - loss: 1.0912\n",
    "\n",
    "Epoch 00063: loss improved from 1.15217 to 1.14188, saving model to bb_att_only.hdf5\n",
    "Epoch 64/130\n",
    "571/571 [==============================] - 130s 228ms/step - loss: 1.0615\n",
    "\n",
    "Epoch 00064: loss improved from 1.14188 to 1.10729, saving model to bb_att_only.hdf5\n",
    "Epoch 65/130\n",
    "571/571 [==============================] - 130s 228ms/step - loss: 1.0710\n",
    "\n",
    "Epoch 00065: loss improved from 1.10729 to 1.09914, saving model to bb_att_only.hdf5\n",
    "Epoch 66/130\n",
    "571/571 [==============================] - 130s 228ms/step - loss: 1.0256\n",
    "\n",
    "Epoch 00066: loss improved from 1.09914 to 1.07782, saving model to bb_att_only.hdf5\n",
    "Epoch 67/130\n",
    "571/571 [==============================] - 130s 228ms/step - loss: 1.0089\n",
    "\n",
    "Epoch 00067: loss improved from 1.07782 to 1.04482, saving model to bb_att_only.hdf5\n",
    "Epoch 68/130\n",
    "571/571 [==============================] - 130s 228ms/step - loss: 0.9938\n",
    "\n",
    "Epoch 00068: loss did not improve from 1.04482\n",
    "Epoch 69/130\n",
    "571/571 [==============================] - 130s 228ms/step - loss: 0.9906\n",
    "\n",
    "Epoch 00069: loss did not improve from 1.04482\n",
    "Epoch 70/130\n",
    "571/571 [==============================] - 130s 228ms/step - loss: 0.9672\n",
    "\n",
    "Epoch 00070: loss improved from 1.04482 to 1.00589, saving model to bb_att_only.hdf5\n",
    "Epoch 71/130\n",
    "571/571 [==============================] - 130s 228ms/step - loss: 0.9394\n",
    "\n",
    "Epoch 00071: loss improved from 1.00589 to 0.98496, saving model to bb_att_only.hdf5\n",
    "Epoch 72/130\n",
    "571/571 [==============================] - 130s 228ms/step - loss: 0.9282\n",
    "\n",
    "Epoch 00072: loss improved from 0.98496 to 0.97041, saving model to bb_att_only.hdf5\n",
    "Epoch 73/130\n",
    "571/571 [==============================] - 130s 228ms/step - loss: 0.9062\n",
    "\n",
    "Epoch 00073: loss improved from 0.97041 to 0.95029, saving model to bb_att_only.hdf5\n",
    "Epoch 74/130\n",
    "571/571 [==============================] - 130s 228ms/step - loss: 0.8948\n",
    "\n",
    "Epoch 00074: loss improved from 0.95029 to 0.94537, saving model to bb_att_only.hdf5\n",
    "Epoch 75/130\n",
    "571/571 [==============================] - 130s 228ms/step - loss: 0.8787\n",
    "\n",
    "Epoch 00075: loss improved from 0.94537 to 0.91519, saving model to bb_att_only.hdf5\n",
    "Epoch 76/130\n",
    "571/571 [==============================] - 130s 228ms/step - loss: 0.8688\n",
    "\n",
    "Epoch 00076: loss improved from 0.91519 to 0.90910, saving model to bb_att_only.hdf5\n",
    "Epoch 77/130\n",
    "571/571 [==============================] - 130s 227ms/step - loss: 0.8473\n",
    "\n",
    "Epoch 00077: loss improved from 0.90910 to 0.89178, saving model to bb_att_only.hdf5\n",
    "Epoch 78/130\n",
    "571/571 [==============================] - 130s 227ms/step - loss: 0.8503\n",
    "\n",
    "Epoch 00078: loss improved from 0.89178 to 0.88099, saving model to bb_att_only.hdf5\n",
    "Epoch 79/130\n",
    "571/571 [==============================] - 130s 227ms/step - loss: 0.8191\n",
    "\n",
    "Epoch 00079: loss improved from 0.88099 to 0.85650, saving model to bb_att_only.hdf5\n",
    "Epoch 80/130\n",
    "571/571 [==============================] - 130s 227ms/step - loss: 0.8099\n",
    "\n",
    "Epoch 00080: loss improved from 0.85650 to 0.85447, saving model to bb_att_only.hdf5\n",
    "Epoch 81/130\n",
    "571/571 [==============================] - 130s 227ms/step - loss: 0.8013\n",
    "\n",
    "Epoch 00081: loss improved from 0.85447 to 0.84990, saving model to bb_att_only.hdf5\n",
    "Epoch 82/130\n",
    "571/571 [==============================] - 130s 228ms/step - loss: 0.8185\n",
    "\n",
    "Epoch 00082: loss improved from 0.84990 to 0.83055, saving model to bb_att_only.hdf5\n",
    "Epoch 83/130\n",
    "571/571 [==============================] - 130s 228ms/step - loss: 0.8027\n",
    "\n",
    "Epoch 00083: loss improved from 0.83055 to 0.82950, saving model to bb_att_only.hdf5\n",
    "Epoch 84/130\n",
    "571/571 [==============================] - 130s 228ms/step - loss: 0.7862\n",
    "\n",
    "Epoch 00084: loss improved from 0.82950 to 0.81645, saving model to bb_att_only.hdf5\n",
    "Epoch 85/130\n",
    "571/571 [==============================] - 130s 228ms/step - loss: 0.7602\n",
    "\n",
    "Epoch 00085: loss improved from 0.81645 to 0.78775, saving model to bb_att_only.hdf5\n",
    "Epoch 86/130\n",
    "571/571 [==============================] - 130s 228ms/step - loss: 0.7479\n",
    "\n",
    "Epoch 00086: loss did not improve from 0.78775\n",
    "Epoch 87/130\n",
    "571/571 [==============================] - 131s 229ms/step - loss: 0.7603\n",
    "\n",
    "Epoch 00087: loss improved from 0.78775 to 0.78241, saving model to bb_att_only.hdf5\n",
    "Epoch 88/130\n",
    "571/571 [==============================] - 131s 229ms/step - loss: 0.7045\n",
    "\n",
    "Epoch 00088: loss improved from 0.78241 to 0.74656, saving model to bb_att_only.hdf5\n",
    "Epoch 89/130\n",
    "571/571 [==============================] - 131s 229ms/step - loss: 0.7118\n",
    "\n",
    "Epoch 00089: loss did not improve from 0.74656\n",
    "Epoch 90/130\n",
    "571/571 [==============================] - 131s 229ms/step - loss: 0.7190\n",
    "\n",
    "Epoch 00090: loss did not improve from 0.74656\n",
    "Epoch 91/130\n",
    "571/571 [==============================] - 131s 229ms/step - loss: 0.6883\n",
    "\n",
    "Epoch 00091: loss improved from 0.74656 to 0.73599, saving model to bb_att_only.hdf5\n",
    "Epoch 92/130\n",
    "571/571 [==============================] - 131s 229ms/step - loss: 0.6916\n",
    "\n",
    "Epoch 00092: loss improved from 0.73599 to 0.72684, saving model to bb_att_only.hdf5\n",
    "Epoch 93/130\n",
    "571/571 [==============================] - 131s 229ms/step - loss: 0.6748\n",
    "\n",
    "Epoch 00093: loss improved from 0.72684 to 0.70261, saving model to bb_att_only.hdf5\n",
    "Epoch 94/130\n",
    "571/571 [==============================] - 131s 229ms/step - loss: 0.6664\n",
    "\n",
    "Epoch 00094: loss improved from 0.70261 to 0.69936, saving model to bb_att_only.hdf5\n",
    "Epoch 95/130\n",
    "571/571 [==============================] - 131s 229ms/step - loss: 0.6640\n",
    "\n",
    "Epoch 00095: loss improved from 0.69936 to 0.69769, saving model to bb_att_only.hdf5\n",
    "Epoch 96/130\n",
    "571/571 [==============================] - 131s 229ms/step - loss: 0.6854\n",
    "\n",
    "Epoch 00096: loss did not improve from 0.69769\n",
    "Epoch 97/130\n",
    "571/571 [==============================] - 131s 229ms/step - loss: 0.6731\n",
    "\n",
    "Epoch 00097: loss did not improve from 0.69769\n",
    "Epoch 98/130\n",
    "571/571 [==============================] - 131s 229ms/step - loss: 0.6621\n",
    "\n",
    "Epoch 00098: loss did not improve from 0.69769\n",
    "Epoch 99/130\n",
    "571/571 [==============================] - 131s 229ms/step - loss: 0.6670\n",
    "\n",
    "Epoch 00099: loss improved from 0.69769 to 0.68180, saving model to bb_att_only.hdf5\n",
    "Epoch 100/130\n",
    "571/571 [==============================] - 130s 228ms/step - loss: 0.6386\n",
    "\n",
    "Epoch 00100: loss improved from 0.68180 to 0.67454, saving model to bb_att_only.hdf5\n",
    "Epoch 101/130\n",
    "571/571 [==============================] - 130s 228ms/step - loss: 0.6290\n",
    "\n",
    "Epoch 00101: loss improved from 0.67454 to 0.65924, saving model to bb_att_only.hdf5\n",
    "Epoch 102/130\n",
    "571/571 [==============================] - 130s 229ms/step - loss: 0.6253\n",
    "\n",
    "Epoch 00102: loss improved from 0.65924 to 0.65647, saving model to bb_att_only.hdf5\n",
    "Epoch 103/130\n",
    "571/571 [==============================] - 131s 229ms/step - loss: 0.6347\n",
    "\n",
    "Epoch 00103: loss did not improve from 0.65647\n",
    "Epoch 104/130\n",
    "571/571 [==============================] - 131s 229ms/step - loss: 0.6117\n",
    "\n",
    "Epoch 00104: loss improved from 0.65647 to 0.64529, saving model to bb_att_only.hdf5\n",
    "Epoch 105/130\n",
    "571/571 [==============================] - 130s 228ms/step - loss: 0.6094\n",
    "\n",
    "Epoch 00105: loss improved from 0.64529 to 0.63123, saving model to bb_att_only.hdf5\n",
    "Epoch 106/130\n",
    "571/571 [==============================] - 130s 228ms/step - loss: 0.6150\n",
    "\n",
    "Epoch 00106: loss did not improve from 0.63123\n",
    "Epoch 107/130\n",
    "571/571 [==============================] - 130s 228ms/step - loss: 0.5944\n",
    "\n",
    "Epoch 00107: loss improved from 0.63123 to 0.62622, saving model to bb_att_only.hdf5\n",
    "Epoch 108/130\n",
    "571/571 [==============================] - 131s 229ms/step - loss: 0.6048\n",
    "\n",
    "Epoch 00108: loss did not improve from 0.62622\n",
    "Epoch 109/130\n",
    "571/571 [==============================] - 131s 229ms/step - loss: 0.5888\n",
    "\n",
    "Epoch 00109: loss improved from 0.62622 to 0.62586, saving model to bb_att_only.hdf5\n",
    "Epoch 110/130\n",
    "571/571 [==============================] - 131s 229ms/step - loss: 0.5748\n",
    "\n",
    "Epoch 00110: loss improved from 0.62586 to 0.60347, saving model to bb_att_only.hdf5\n",
    "Epoch 111/130\n",
    "571/571 [==============================] - 131s 229ms/step - loss: 0.5806\n",
    "\n",
    "Epoch 00111: loss did not improve from 0.60347\n",
    "Epoch 112/130\n",
    "571/571 [==============================] - 131s 229ms/step - loss: 0.5620\n",
    "\n",
    "Epoch 00112: loss improved from 0.60347 to 0.59973, saving model to bb_att_only.hdf5\n",
    "Epoch 113/130\n",
    "571/571 [==============================] - 131s 229ms/step - loss: 0.5616\n",
    "\n",
    "Epoch 00113: loss improved from 0.59973 to 0.59894, saving model to bb_att_only.hdf5\n",
    "Epoch 114/130\n",
    "571/571 [==============================] - 131s 230ms/step - loss: 0.5652\n",
    "\n",
    "Epoch 00114: loss improved from 0.59894 to 0.59504, saving model to bb_att_only.hdf5\n",
    "Epoch 115/130\n",
    "571/571 [==============================] - 131s 229ms/step - loss: 0.5552\n",
    "\n",
    "Epoch 00115: loss improved from 0.59504 to 0.58480, saving model to bb_att_only.hdf5\n",
    "Epoch 116/130\n",
    "571/571 [==============================] - 131s 229ms/step - loss: 0.5668\n",
    "\n",
    "Epoch 00116: loss did not improve from 0.58480\n",
    "Epoch 117/130\n",
    "571/571 [==============================] - 131s 229ms/step - loss: 0.5522\n",
    "\n",
    "Epoch 00117: loss improved from 0.58480 to 0.57092, saving model to bb_att_only.hdf5\n",
    "Epoch 118/130\n",
    "571/571 [==============================] - 131s 229ms/step - loss: 0.5328\n",
    "\n",
    "Epoch 00118: loss did not improve from 0.57092\n",
    "Epoch 119/130\n",
    "571/571 [==============================] - 131s 229ms/step - loss: 0.5384\n",
    "\n",
    "Epoch 00119: loss improved from 0.57092 to 0.56386, saving model to bb_att_only.hdf5\n",
    "Epoch 120/130\n",
    "571/571 [==============================] - 131s 229ms/step - loss: 0.5445\n",
    "\n",
    "Epoch 00120: loss did not improve from 0.56386\n",
    "Epoch 121/130\n",
    "571/571 [==============================] - 131s 229ms/step - loss: 0.5258\n",
    "\n",
    "Epoch 00121: loss improved from 0.56386 to 0.55588, saving model to bb_att_only.hdf5\n",
    "Epoch 122/130\n",
    "571/571 [==============================] - 131s 230ms/step - loss: 0.5414\n",
    "\n",
    "Epoch 00122: loss did not improve from 0.55588\n",
    "Epoch 123/130\n",
    "571/571 [==============================] - 131s 230ms/step - loss: 0.5476\n",
    "\n",
    "Epoch 00123: loss did not improve from 0.55588\n",
    "Epoch 124/130\n",
    "571/571 [==============================] - 131s 230ms/step - loss: 0.5354\n",
    "\n",
    "Epoch 00124: loss did not improve from 0.55588\n",
    "Epoch 125/130\n",
    "571/571 [==============================] - 131s 230ms/step - loss: 0.5346\n",
    "\n",
    "Epoch 00125: loss improved from 0.55588 to 0.55375, saving model to bb_att_only.hdf5\n",
    "Epoch 126/130\n",
    "571/571 [==============================] - 131s 230ms/step - loss: 0.5023\n",
    "\n",
    "Epoch 00126: loss improved from 0.55375 to 0.54018, saving model to bb_att_only.hdf5\n",
    "Epoch 127/130\n",
    "571/571 [==============================] - 131s 230ms/step - loss: 0.5312\n",
    "\n",
    "Epoch 00127: loss did not improve from 0.54018\n",
    "Epoch 128/130\n",
    "571/571 [==============================] - 131s 230ms/step - loss: 0.5193\n",
    "\n",
    "Epoch 00128: loss did not improve from 0.54018\n",
    "Epoch 129/130\n",
    "571/571 [==============================] - 131s 230ms/step - loss: 0.5216\n",
    "\n",
    "Epoch 00129: loss did not improve from 0.54018\n",
    "Epoch 130/130\n",
    "571/571 [==============================] - 131s 229ms/step - loss: 0.5075\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_att_time = '''Epoch 1/130\n",
    "508/508 [==============================] - 102s 179ms/step - loss: 5.8541\n",
    "\n",
    "Epoch 00001: loss improved from inf to 5.71275, saving model to bb_att_time.hdf5\n",
    "Epoch 2/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 5.6386\n",
    "\n",
    "Epoch 00002: loss improved from 5.71275 to 5.60675, saving model to bb_att_time.hdf5\n",
    "Epoch 3/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 5.5332\n",
    "\n",
    "Epoch 00003: loss improved from 5.60675 to 5.53229, saving model to bb_att_time.hdf5\n",
    "Epoch 4/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 5.4669\n",
    "\n",
    "Epoch 00004: loss improved from 5.53229 to 5.46457, saving model to bb_att_time.hdf5\n",
    "Epoch 5/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 5.4064\n",
    "\n",
    "Epoch 00005: loss improved from 5.46457 to 5.40943, saving model to bb_att_time.hdf5\n",
    "Epoch 6/130\n",
    "508/508 [==============================] - 91s 178ms/step - loss: 5.3517\n",
    "\n",
    "Epoch 00006: loss improved from 5.40943 to 5.34691, saving model to bb_att_time.hdf5\n",
    "Epoch 7/130\n",
    "508/508 [==============================] - 91s 178ms/step - loss: 5.2513\n",
    "\n",
    "Epoch 00007: loss improved from 5.34691 to 5.25557, saving model to bb_att_time.hdf5\n",
    "Epoch 8/130\n",
    "508/508 [==============================] - 90s 178ms/step - loss: 5.1582\n",
    "\n",
    "Epoch 00008: loss improved from 5.25557 to 5.15209, saving model to bb_att_time.hdf5\n",
    "Epoch 9/130\n",
    "508/508 [==============================] - 91s 178ms/step - loss: 5.0664\n",
    "\n",
    "Epoch 00009: loss improved from 5.15209 to 5.07724, saving model to bb_att_time.hdf5\n",
    "Epoch 10/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 4.9788\n",
    "\n",
    "Epoch 00010: loss improved from 5.07724 to 4.99815, saving model to bb_att_time.hdf5\n",
    "Epoch 11/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 4.9173\n",
    "\n",
    "Epoch 00011: loss improved from 4.99815 to 4.92008, saving model to bb_att_time.hdf5\n",
    "Epoch 12/130\n",
    "508/508 [==============================] - 91s 178ms/step - loss: 4.8232\n",
    "\n",
    "Epoch 00012: loss improved from 4.92008 to 4.83018, saving model to bb_att_time.hdf5\n",
    "Epoch 13/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 4.7328\n",
    "\n",
    "Epoch 00013: loss improved from 4.83018 to 4.72967, saving model to bb_att_time.hdf5\n",
    "Epoch 14/130\n",
    "508/508 [==============================] - 90s 178ms/step - loss: 4.6243\n",
    "\n",
    "Epoch 00014: loss improved from 4.72967 to 4.60597, saving model to bb_att_time.hdf5\n",
    "Epoch 15/130\n",
    "508/508 [==============================] - 91s 178ms/step - loss: 4.5087\n",
    "\n",
    "Epoch 00015: loss improved from 4.60597 to 4.50832, saving model to bb_att_time.hdf5\n",
    "Epoch 16/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 4.3966\n",
    "\n",
    "Epoch 00016: loss improved from 4.50832 to 4.41609, saving model to bb_att_time.hdf5\n",
    "Epoch 17/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 4.2845\n",
    "\n",
    "Epoch 00017: loss improved from 4.41609 to 4.31467, saving model to bb_att_time.hdf5\n",
    "Epoch 18/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 4.1903\n",
    "\n",
    "Epoch 00018: loss improved from 4.31467 to 4.20175, saving model to bb_att_time.hdf5\n",
    "Epoch 19/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 4.0599\n",
    "\n",
    "Epoch 00019: loss improved from 4.20175 to 4.08565, saving model to bb_att_time.hdf5\n",
    "Epoch 20/130\n",
    "508/508 [==============================] - 90s 178ms/step - loss: 3.9411\n",
    "\n",
    "Epoch 00020: loss improved from 4.08565 to 3.97883, saving model to bb_att_time.hdf5\n",
    "Epoch 21/130\n",
    "508/508 [==============================] - 91s 180ms/step - loss: 3.8732\n",
    "\n",
    "Epoch 00021: loss improved from 3.97883 to 3.88118, saving model to bb_att_time.hdf5\n",
    "Epoch 22/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 3.7416\n",
    "\n",
    "Epoch 00022: loss improved from 3.88118 to 3.76418, saving model to bb_att_time.hdf5\n",
    "Epoch 23/130\n",
    "508/508 [==============================] - 91s 178ms/step - loss: 3.6348\n",
    "\n",
    "Epoch 00023: loss improved from 3.76418 to 3.66146, saving model to bb_att_time.hdf5\n",
    "Epoch 24/130\n",
    "508/508 [==============================] - 91s 180ms/step - loss: 3.5235\n",
    "\n",
    "Epoch 00024: loss improved from 3.66146 to 3.56279, saving model to bb_att_time.hdf5\n",
    "Epoch 25/130\n",
    "508/508 [==============================] - 90s 177ms/step - loss: 3.4356\n",
    "\n",
    "Epoch 00025: loss improved from 3.56279 to 3.47719, saving model to bb_att_time.hdf5\n",
    "Epoch 26/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 3.3512\n",
    "\n",
    "Epoch 00026: loss improved from 3.47719 to 3.39145, saving model to bb_att_time.hdf5\n",
    "Epoch 27/130\n",
    "508/508 [==============================] - 91s 178ms/step - loss: 3.2534\n",
    "\n",
    "Epoch 00027: loss improved from 3.39145 to 3.29683, saving model to bb_att_time.hdf5\n",
    "Epoch 28/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 3.1675\n",
    "\n",
    "Epoch 00028: loss improved from 3.29683 to 3.22776, saving model to bb_att_time.hdf5\n",
    "Epoch 29/130\n",
    "508/508 [==============================] - 91s 180ms/step - loss: 3.1147\n",
    "\n",
    "Epoch 00029: loss improved from 3.22776 to 3.14058, saving model to bb_att_time.hdf5\n",
    "Epoch 30/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 3.0276\n",
    "\n",
    "Epoch 00030: loss improved from 3.14058 to 3.05912, saving model to bb_att_time.hdf5\n",
    "Epoch 31/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 2.9228\n",
    "\n",
    "Epoch 00031: loss improved from 3.05912 to 2.96516, saving model to bb_att_time.hdf5\n",
    "Epoch 32/130\n",
    "508/508 [==============================] - 91s 180ms/step - loss: 2.8544\n",
    "\n",
    "Epoch 00032: loss improved from 2.96516 to 2.88699, saving model to bb_att_time.hdf5\n",
    "Epoch 33/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 2.7523\n",
    "\n",
    "Epoch 00033: loss improved from 2.88699 to 2.81848, saving model to bb_att_time.hdf5\n",
    "Epoch 34/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 2.6996\n",
    "\n",
    "Epoch 00034: loss improved from 2.81848 to 2.74598, saving model to bb_att_time.hdf5\n",
    "Epoch 35/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 2.5966\n",
    "\n",
    "Epoch 00035: loss improved from 2.74598 to 2.64987, saving model to bb_att_time.hdf5\n",
    "Epoch 36/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 2.5110\n",
    "\n",
    "Epoch 00036: loss improved from 2.64987 to 2.56553, saving model to bb_att_time.hdf5\n",
    "Epoch 37/130\n",
    "508/508 [==============================] - 91s 178ms/step - loss: 2.4432\n",
    "\n",
    "Epoch 00037: loss improved from 2.56553 to 2.49429, saving model to bb_att_time.hdf5\n",
    "Epoch 38/130\n",
    "508/508 [==============================] - 91s 180ms/step - loss: 2.3653\n",
    "\n",
    "Epoch 00038: loss improved from 2.49429 to 2.39898, saving model to bb_att_time.hdf5\n",
    "Epoch 39/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 2.2601\n",
    "\n",
    "Epoch 00039: loss improved from 2.39898 to 2.32273, saving model to bb_att_time.hdf5\n",
    "Epoch 40/130\n",
    "508/508 [==============================] - 91s 180ms/step - loss: 2.2097\n",
    "\n",
    "Epoch 00040: loss improved from 2.32273 to 2.26986, saving model to bb_att_time.hdf5\n",
    "Epoch 41/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 2.1385\n",
    "\n",
    "Epoch 00041: loss improved from 2.26986 to 2.19935, saving model to bb_att_time.hdf5\n",
    "Epoch 42/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 2.0947\n",
    "\n",
    "Epoch 00042: loss improved from 2.19935 to 2.13887, saving model to bb_att_time.hdf5\n",
    "Epoch 43/130\n",
    "508/508 [==============================] - 91s 178ms/step - loss: 2.0234\n",
    "\n",
    "Epoch 00043: loss improved from 2.13887 to 2.07817, saving model to bb_att_time.hdf5\n",
    "Epoch 44/130\n",
    "508/508 [==============================] - 91s 178ms/step - loss: 1.9525\n",
    "\n",
    "Epoch 00044: loss improved from 2.07817 to 2.00898, saving model to bb_att_time.hdf5\n",
    "Epoch 45/130\n",
    "508/508 [==============================] - 90s 178ms/step - loss: 1.8863\n",
    "\n",
    "Epoch 00045: loss improved from 2.00898 to 1.94128, saving model to bb_att_time.hdf5\n",
    "Epoch 46/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 1.8301\n",
    "\n",
    "Epoch 00046: loss improved from 1.94128 to 1.87467, saving model to bb_att_time.hdf5\n",
    "Epoch 47/130\n",
    "508/508 [==============================] - 90s 178ms/step - loss: 1.7579\n",
    "\n",
    "Epoch 00047: loss improved from 1.87467 to 1.81651, saving model to bb_att_time.hdf5\n",
    "Epoch 48/130\n",
    "508/508 [==============================] - 91s 180ms/step - loss: 1.6926\n",
    "\n",
    "Epoch 00048: loss improved from 1.81651 to 1.74570, saving model to bb_att_time.hdf5\n",
    "Epoch 49/130\n",
    "508/508 [==============================] - 91s 180ms/step - loss: 1.6546\n",
    "\n",
    "Epoch 00049: loss improved from 1.74570 to 1.69959, saving model to bb_att_time.hdf5\n",
    "Epoch 50/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 1.5720\n",
    "\n",
    "Epoch 00050: loss improved from 1.69959 to 1.65349, saving model to bb_att_time.hdf5\n",
    "Epoch 51/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 1.5557\n",
    "\n",
    "Epoch 00051: loss improved from 1.65349 to 1.61042, saving model to bb_att_time.hdf5\n",
    "Epoch 52/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 1.5178\n",
    "\n",
    "Epoch 00052: loss improved from 1.61042 to 1.55225, saving model to bb_att_time.hdf5\n",
    "Epoch 53/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 1.4727\n",
    "\n",
    "Epoch 00053: loss improved from 1.55225 to 1.50888, saving model to bb_att_time.hdf5\n",
    "Epoch 54/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 1.4267\n",
    "\n",
    "Epoch 00054: loss improved from 1.50888 to 1.46967, saving model to bb_att_time.hdf5\n",
    "Epoch 55/130\n",
    "508/508 [==============================] - 91s 180ms/step - loss: 1.3606\n",
    "\n",
    "Epoch 00055: loss improved from 1.46967 to 1.41797, saving model to bb_att_time.hdf5\n",
    "Epoch 56/130\n",
    "508/508 [==============================] - 91s 178ms/step - loss: 1.2963\n",
    "\n",
    "Epoch 00056: loss improved from 1.41797 to 1.34736, saving model to bb_att_time.hdf5\n",
    "Epoch 57/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 1.2797\n",
    "\n",
    "Epoch 00057: loss improved from 1.34736 to 1.32260, saving model to bb_att_time.hdf5\n",
    "Epoch 58/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 1.2714\n",
    "\n",
    "Epoch 00058: loss improved from 1.32260 to 1.30622, saving model to bb_att_time.hdf5\n",
    "Epoch 59/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 1.2238\n",
    "\n",
    "Epoch 00059: loss improved from 1.30622 to 1.26944, saving model to bb_att_time.hdf5\n",
    "Epoch 60/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 1.1875\n",
    "\n",
    "Epoch 00060: loss improved from 1.26944 to 1.23053, saving model to bb_att_time.hdf5\n",
    "Epoch 61/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 1.1554\n",
    "\n",
    "Epoch 00061: loss improved from 1.23053 to 1.20428, saving model to bb_att_time.hdf5\n",
    "Epoch 62/130\n",
    "508/508 [==============================] - 91s 180ms/step - loss: 1.1371\n",
    "\n",
    "Epoch 00062: loss improved from 1.20428 to 1.17556, saving model to bb_att_time.hdf5\n",
    "Epoch 63/130\n",
    "508/508 [==============================] - 91s 180ms/step - loss: 1.1063\n",
    "\n",
    "Epoch 00063: loss improved from 1.17556 to 1.14120, saving model to bb_att_time.hdf5\n",
    "Epoch 64/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 1.0864\n",
    "\n",
    "Epoch 00064: loss improved from 1.14120 to 1.11850, saving model to bb_att_time.hdf5\n",
    "Epoch 65/130\n",
    "508/508 [==============================] - 91s 180ms/step - loss: 1.0484\n",
    "\n",
    "Epoch 00065: loss improved from 1.11850 to 1.09001, saving model to bb_att_time.hdf5\n",
    "Epoch 66/130\n",
    "508/508 [==============================] - 91s 180ms/step - loss: 1.0234\n",
    "\n",
    "Epoch 00066: loss improved from 1.09001 to 1.05869, saving model to bb_att_time.hdf5\n",
    "Epoch 67/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 1.0164\n",
    "\n",
    "Epoch 00067: loss improved from 1.05869 to 1.04414, saving model to bb_att_time.hdf5\n",
    "Epoch 68/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 0.9439\n",
    "\n",
    "Epoch 00068: loss improved from 1.04414 to 0.99711, saving model to bb_att_time.hdf5\n",
    "Epoch 69/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 0.9600\n",
    "\n",
    "Epoch 00069: loss improved from 0.99711 to 0.98940, saving model to bb_att_time.hdf5\n",
    "Epoch 70/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 0.9047\n",
    "\n",
    "Epoch 00070: loss improved from 0.98940 to 0.95614, saving model to bb_att_time.hdf5\n",
    "Epoch 71/130\n",
    "508/508 [==============================] - 91s 180ms/step - loss: 0.9226\n",
    "\n",
    "Epoch 00071: loss improved from 0.95614 to 0.95309, saving model to bb_att_time.hdf5\n",
    "Epoch 72/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 0.8962\n",
    "\n",
    "Epoch 00072: loss improved from 0.95309 to 0.93007, saving model to bb_att_time.hdf5\n",
    "Epoch 73/130\n",
    "508/508 [==============================] - 90s 178ms/step - loss: 0.8628\n",
    "\n",
    "Epoch 00073: loss improved from 0.93007 to 0.89720, saving model to bb_att_time.hdf5\n",
    "Epoch 74/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 0.8351\n",
    "\n",
    "Epoch 00074: loss improved from 0.89720 to 0.86938, saving model to bb_att_time.hdf5\n",
    "Epoch 75/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 0.8085\n",
    "\n",
    "Epoch 00075: loss improved from 0.86938 to 0.84857, saving model to bb_att_time.hdf5\n",
    "Epoch 76/130\n",
    "508/508 [==============================] - 91s 180ms/step - loss: 0.8018\n",
    "\n",
    "Epoch 00076: loss improved from 0.84857 to 0.84788, saving model to bb_att_time.hdf5\n",
    "Epoch 77/130\n",
    "508/508 [==============================] - 91s 180ms/step - loss: 0.8023\n",
    "\n",
    "Epoch 00077: loss improved from 0.84788 to 0.83477, saving model to bb_att_time.hdf5\n",
    "Epoch 78/130\n",
    "508/508 [==============================] - 92s 180ms/step - loss: 0.7671\n",
    "\n",
    "Epoch 00078: loss improved from 0.83477 to 0.81281, saving model to bb_att_time.hdf5\n",
    "Epoch 79/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 0.7718\n",
    "\n",
    "Epoch 00079: loss improved from 0.81281 to 0.81053, saving model to bb_att_time.hdf5\n",
    "Epoch 80/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 0.7592\n",
    "\n",
    "Epoch 00080: loss improved from 0.81053 to 0.78524, saving model to bb_att_time.hdf5\n",
    "Epoch 81/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 0.7524\n",
    "\n",
    "Epoch 00081: loss improved from 0.78524 to 0.77265, saving model to bb_att_time.hdf5\n",
    "Epoch 82/130\n",
    "508/508 [==============================] - 91s 180ms/step - loss: 0.6994\n",
    "\n",
    "Epoch 00082: loss improved from 0.77265 to 0.73949, saving model to bb_att_time.hdf5\n",
    "Epoch 83/130\n",
    "508/508 [==============================] - 91s 180ms/step - loss: 0.7182\n",
    "\n",
    "Epoch 00083: loss did not improve from 0.73949\n",
    "Epoch 84/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 0.6913\n",
    "\n",
    "Epoch 00084: loss improved from 0.73949 to 0.72604, saving model to bb_att_time.hdf5\n",
    "Epoch 85/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 0.6784\n",
    "\n",
    "Epoch 00085: loss improved from 0.72604 to 0.71726, saving model to bb_att_time.hdf5\n",
    "Epoch 86/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 0.6768\n",
    "\n",
    "Epoch 00086: loss improved from 0.71726 to 0.70671, saving model to bb_att_time.hdf5\n",
    "Epoch 87/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 0.6594\n",
    "\n",
    "Epoch 00087: loss improved from 0.70671 to 0.68282, saving model to bb_att_time.hdf5\n",
    "Epoch 88/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 0.6591\n",
    "\n",
    "Epoch 00088: loss improved from 0.68282 to 0.68230, saving model to bb_att_time.hdf5\n",
    "Epoch 89/130\n",
    "508/508 [==============================] - 92s 180ms/step - loss: 0.6526\n",
    "\n",
    "Epoch 00089: loss improved from 0.68230 to 0.66676, saving model to bb_att_time.hdf5\n",
    "Epoch 90/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 0.6454\n",
    "\n",
    "Epoch 00090: loss improved from 0.66676 to 0.65809, saving model to bb_att_time.hdf5\n",
    "Epoch 91/130\n",
    "508/508 [==============================] - 92s 180ms/step - loss: 0.6052\n",
    "\n",
    "Epoch 00091: loss improved from 0.65809 to 0.63879, saving model to bb_att_time.hdf5\n",
    "Epoch 92/130\n",
    "508/508 [==============================] - 92s 181ms/step - loss: 0.5979\n",
    "\n",
    "Epoch 00092: loss improved from 0.63879 to 0.63373, saving model to bb_att_time.hdf5\n",
    "Epoch 93/130\n",
    "508/508 [==============================] - 92s 180ms/step - loss: 0.5896\n",
    "\n",
    "Epoch 00093: loss improved from 0.63373 to 0.62089, saving model to bb_att_time.hdf5\n",
    "Epoch 94/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 0.5928\n",
    "\n",
    "Epoch 00094: loss improved from 0.62089 to 0.61529, saving model to bb_att_time.hdf5\n",
    "Epoch 95/130\n",
    "508/508 [==============================] - 91s 180ms/step - loss: 0.5778\n",
    "\n",
    "Epoch 00095: loss improved from 0.61529 to 0.60317, saving model to bb_att_time.hdf5\n",
    "Epoch 96/130\n",
    "508/508 [==============================] - 92s 180ms/step - loss: 0.5683\n",
    "\n",
    "Epoch 00096: loss improved from 0.60317 to 0.59667, saving model to bb_att_time.hdf5\n",
    "Epoch 97/130\n",
    "508/508 [==============================] - 91s 180ms/step - loss: 0.5575\n",
    "\n",
    "Epoch 00097: loss improved from 0.59667 to 0.59479, saving model to bb_att_time.hdf5\n",
    "Epoch 98/130\n",
    "508/508 [==============================] - 91s 180ms/step - loss: 0.5687\n",
    "\n",
    "Epoch 00098: loss improved from 0.59479 to 0.58989, saving model to bb_att_time.hdf5\n",
    "Epoch 99/130\n",
    "508/508 [==============================] - 92s 180ms/step - loss: 0.5543\n",
    "\n",
    "Epoch 00099: loss improved from 0.58989 to 0.56817, saving model to bb_att_time.hdf5\n",
    "Epoch 100/130\n",
    "508/508 [==============================] - 92s 180ms/step - loss: 0.5472\n",
    "\n",
    "Epoch 00100: loss did not improve from 0.56817\n",
    "Epoch 101/130\n",
    "508/508 [==============================] - 92s 180ms/step - loss: 0.5564\n",
    "\n",
    "Epoch 00101: loss improved from 0.56817 to 0.56301, saving model to bb_att_time.hdf5\n",
    "Epoch 102/130\n",
    "508/508 [==============================] - 91s 180ms/step - loss: 0.5407\n",
    "\n",
    "Epoch 00102: loss improved from 0.56301 to 0.54930, saving model to bb_att_time.hdf5\n",
    "Epoch 103/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 0.5339\n",
    "\n",
    "Epoch 00103: loss did not improve from 0.54930\n",
    "Epoch 104/130\n",
    "508/508 [==============================] - 91s 180ms/step - loss: 0.5298\n",
    "\n",
    "Epoch 00104: loss improved from 0.54930 to 0.54406, saving model to bb_att_time.hdf5\n",
    "Epoch 105/130\n",
    "508/508 [==============================] - 91s 178ms/step - loss: 0.5218\n",
    "\n",
    "Epoch 00105: loss improved from 0.54406 to 0.53949, saving model to bb_att_time.hdf5\n",
    "Epoch 106/130\n",
    "508/508 [==============================] - 91s 178ms/step - loss: 0.5206\n",
    "\n",
    "Epoch 00106: loss improved from 0.53949 to 0.53515, saving model to bb_att_time.hdf5\n",
    "Epoch 107/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 0.5078\n",
    "\n",
    "Epoch 00107: loss improved from 0.53515 to 0.53192, saving model to bb_att_time.hdf5\n",
    "Epoch 108/130\n",
    "508/508 [==============================] - 91s 180ms/step - loss: 0.4936\n",
    "\n",
    "Epoch 00108: loss improved from 0.53192 to 0.51527, saving model to bb_att_time.hdf5\n",
    "Epoch 109/130\n",
    "508/508 [==============================] - 91s 180ms/step - loss: 0.4811\n",
    "\n",
    "Epoch 00109: loss improved from 0.51527 to 0.51051, saving model to bb_att_time.hdf5\n",
    "Epoch 110/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 0.5050\n",
    "\n",
    "Epoch 00110: loss did not improve from 0.51051\n",
    "Epoch 111/130\n",
    "508/508 [==============================] - 92s 181ms/step - loss: 0.4803\n",
    "\n",
    "Epoch 00111: loss improved from 0.51051 to 0.50798, saving model to bb_att_time.hdf5\n",
    "Epoch 112/130\n",
    "508/508 [==============================] - 91s 180ms/step - loss: 0.4811\n",
    "\n",
    "Epoch 00112: loss improved from 0.50798 to 0.50026, saving model to bb_att_time.hdf5\n",
    "Epoch 113/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 0.4700\n",
    "\n",
    "Epoch 00113: loss improved from 0.50026 to 0.49773, saving model to bb_att_time.hdf5\n",
    "Epoch 114/130\n",
    "508/508 [==============================] - 91s 178ms/step - loss: 0.4694\n",
    "\n",
    "Epoch 00114: loss improved from 0.49773 to 0.48232, saving model to bb_att_time.hdf5\n",
    "Epoch 115/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 0.4521\n",
    "\n",
    "Epoch 00115: loss improved from 0.48232 to 0.46933, saving model to bb_att_time.hdf5\n",
    "Epoch 116/130\n",
    "508/508 [==============================] - 91s 180ms/step - loss: 0.4617\n",
    "\n",
    "Epoch 00116: loss did not improve from 0.46933\n",
    "Epoch 117/130\n",
    "508/508 [==============================] - 91s 180ms/step - loss: 0.4519\n",
    "\n",
    "Epoch 00117: loss did not improve from 0.46933\n",
    "Epoch 118/130\n",
    "508/508 [==============================] - 91s 180ms/step - loss: 0.4669\n",
    "\n",
    "Epoch 00118: loss did not improve from 0.46933\n",
    "Epoch 119/130\n",
    "508/508 [==============================] - 91s 180ms/step - loss: 0.4566\n",
    "\n",
    "Epoch 00119: loss did not improve from 0.46933\n",
    "Epoch 120/130\n",
    "508/508 [==============================] - 92s 181ms/step - loss: 0.4469\n",
    "\n",
    "Epoch 00120: loss improved from 0.46933 to 0.46788, saving model to bb_att_time.hdf5\n",
    "Epoch 121/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 0.4440\n",
    "\n",
    "Epoch 00121: loss improved from 0.46788 to 0.46063, saving model to bb_att_time.hdf5\n",
    "Epoch 122/130\n",
    "508/508 [==============================] - 91s 180ms/step - loss: 0.4316\n",
    "\n",
    "Epoch 00122: loss improved from 0.46063 to 0.45914, saving model to bb_att_time.hdf5\n",
    "Epoch 123/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 0.4361\n",
    "\n",
    "Epoch 00123: loss did not improve from 0.45914\n",
    "Epoch 124/130\n",
    "508/508 [==============================] - 91s 180ms/step - loss: 0.4396\n",
    "\n",
    "Epoch 00124: loss improved from 0.45914 to 0.45166, saving model to bb_att_time.hdf5\n",
    "Epoch 125/130\n",
    "508/508 [==============================] - 92s 180ms/step - loss: 0.4382\n",
    "\n",
    "Epoch 00125: loss did not improve from 0.45166\n",
    "Epoch 126/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 0.4049\n",
    "\n",
    "Epoch 00126: loss improved from 0.45166 to 0.43530, saving model to bb_att_time.hdf5\n",
    "Epoch 127/130\n",
    "508/508 [==============================] - 91s 180ms/step - loss: 0.4358\n",
    "\n",
    "Epoch 00127: loss did not improve from 0.43530\n",
    "Epoch 128/130\n",
    "508/508 [==============================] - 91s 180ms/step - loss: 0.4225\n",
    "\n",
    "Epoch 00128: loss did not improve from 0.43530\n",
    "Epoch 129/130\n",
    "508/508 [==============================] - 91s 179ms/step - loss: 0.4181\n",
    "\n",
    "Epoch 00129: loss did not improve from 0.43530\n",
    "Epoch 130/130\n",
    "508/508 [==============================] - 91s 180ms/step - loss: 0.4221\n",
    "\n",
    "Epoch 00130: loss did not improve from 0.43530\n",
    "Model: \"sequential_22\"'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# bigger batch no attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_no_att_time_dist = '''Epoch 1/130\n",
    "571/571 [==============================] - 144s 238ms/step - loss: 5.9761\n",
    "\n",
    "Epoch 00001: loss improved from inf to 5.83628, saving model to bigger_batch.hdf5\n",
    "Epoch 2/130\n",
    "571/571 [==============================] - 136s 238ms/step - loss: 5.6606\n",
    "\n",
    "Epoch 00002: loss improved from 5.83628 to 5.63781, saving model to bigger_batch.hdf5\n",
    "Epoch 3/130\n",
    "571/571 [==============================] - 136s 238ms/step - loss: 5.5276\n",
    "\n",
    "Epoch 00003: loss improved from 5.63781 to 5.52618, saving model to bigger_batch.hdf5\n",
    "Epoch 4/130\n",
    "571/571 [==============================] - 136s 238ms/step - loss: 5.3715\n",
    "\n",
    "Epoch 00004: loss improved from 5.52618 to 5.35799, saving model to bigger_batch.hdf5\n",
    "Epoch 5/130\n",
    "571/571 [==============================] - 136s 238ms/step - loss: 5.2116\n",
    "\n",
    "Epoch 00005: loss improved from 5.35799 to 5.18704, saving model to bigger_batch.hdf5\n",
    "Epoch 6/130\n",
    "571/571 [==============================] - 136s 238ms/step - loss: 5.0332\n",
    "\n",
    "Epoch 00006: loss improved from 5.18704 to 5.00840, saving model to bigger_batch.hdf5\n",
    "Epoch 7/130\n",
    "571/571 [==============================] - 136s 238ms/step - loss: 4.8214\n",
    "\n",
    "Epoch 00007: loss improved from 5.00840 to 4.80158, saving model to bigger_batch.hdf5\n",
    "Epoch 8/130\n",
    "571/571 [==============================] - 136s 238ms/step - loss: 4.6084\n",
    "\n",
    "Epoch 00008: loss improved from 4.80158 to 4.62200, saving model to bigger_batch.hdf5\n",
    "Epoch 9/130\n",
    "571/571 [==============================] - 136s 238ms/step - loss: 4.4419\n",
    "\n",
    "Epoch 00009: loss improved from 4.62200 to 4.45785, saving model to bigger_batch.hdf5\n",
    "Epoch 10/130\n",
    "571/571 [==============================] - 136s 238ms/step - loss: 4.2844\n",
    "\n",
    "Epoch 00010: loss improved from 4.45785 to 4.30264, saving model to bigger_batch.hdf5\n",
    "Epoch 11/130\n",
    "571/571 [==============================] - 136s 238ms/step - loss: 4.1232\n",
    "\n",
    "Epoch 00011: loss improved from 4.30264 to 4.13921, saving model to bigger_batch.hdf5\n",
    "Epoch 12/130\n",
    "571/571 [==============================] - 136s 237ms/step - loss: 3.9783\n",
    "\n",
    "Epoch 00012: loss improved from 4.13921 to 4.00295, saving model to bigger_batch.hdf5\n",
    "Epoch 13/130\n",
    "571/571 [==============================] - 136s 237ms/step - loss: 3.8449\n",
    "\n",
    "Epoch 00013: loss improved from 4.00295 to 3.85882, saving model to bigger_batch.hdf5\n",
    "Epoch 14/130\n",
    "571/571 [==============================] - 136s 237ms/step - loss: 3.6905\n",
    "\n",
    "Epoch 00014: loss improved from 3.85882 to 3.71729, saving model to bigger_batch.hdf5\n",
    "Epoch 15/130\n",
    "571/571 [==============================] - 136s 238ms/step - loss: 3.5459\n",
    "\n",
    "Epoch 00015: loss improved from 3.71729 to 3.58237, saving model to bigger_batch.hdf5\n",
    "Epoch 16/130\n",
    "571/571 [==============================] - 136s 237ms/step - loss: 3.4235\n",
    "\n",
    "Epoch 00016: loss improved from 3.58237 to 3.43704, saving model to bigger_batch.hdf5\n",
    "Epoch 17/130\n",
    "571/571 [==============================] - 136s 238ms/step - loss: 3.2176\n",
    "\n",
    "Epoch 00017: loss improved from 3.43704 to 3.29826, saving model to bigger_batch.hdf5\n",
    "Epoch 18/130\n",
    "571/571 [==============================] - 136s 237ms/step - loss: 3.1091\n",
    "\n",
    "Epoch 00018: loss improved from 3.29826 to 3.17412, saving model to bigger_batch.hdf5\n",
    "Epoch 19/130\n",
    "571/571 [==============================] - 136s 238ms/step - loss: 2.9648\n",
    "\n",
    "Epoch 00019: loss improved from 3.17412 to 3.04890, saving model to bigger_batch.hdf5\n",
    "Epoch 20/130\n",
    "571/571 [==============================] - 136s 238ms/step - loss: 2.8768\n",
    "\n",
    "Epoch 00020: loss improved from 3.04890 to 2.93646, saving model to bigger_batch.hdf5\n",
    "Epoch 21/130\n",
    "571/571 [==============================] - 136s 237ms/step - loss: 2.7599\n",
    "\n",
    "Epoch 00021: loss improved from 2.93646 to 2.80267, saving model to bigger_batch.hdf5\n",
    "Epoch 22/130\n",
    "571/571 [==============================] - 136s 237ms/step - loss: 2.5910\n",
    "\n",
    "Epoch 00022: loss improved from 2.80267 to 2.65578, saving model to bigger_batch.hdf5\n",
    "Epoch 23/130\n",
    "571/571 [==============================] - 136s 237ms/step - loss: 2.4668\n",
    "\n",
    "Epoch 00023: loss improved from 2.65578 to 2.55100, saving model to bigger_batch.hdf5\n",
    "Epoch 24/130\n",
    "571/571 [==============================] - 136s 237ms/step - loss: 2.3518\n",
    "\n",
    "Epoch 00024: loss improved from 2.55100 to 2.44751, saving model to bigger_batch.hdf5\n",
    "Epoch 25/130\n",
    "571/571 [==============================] - 135s 237ms/step - loss: 2.2897\n",
    "\n",
    "Epoch 00025: loss improved from 2.44751 to 2.35622, saving model to bigger_batch.hdf5\n",
    "Epoch 26/130\n",
    "571/571 [==============================] - 135s 236ms/step - loss: 2.1610\n",
    "\n",
    "Epoch 00026: loss improved from 2.35622 to 2.24948, saving model to bigger_batch.hdf5\n",
    "Epoch 27/130\n",
    "571/571 [==============================] - 135s 236ms/step - loss: 2.0637\n",
    "\n",
    "Epoch 00027: loss improved from 2.24948 to 2.16069, saving model to bigger_batch.hdf5\n",
    "Epoch 28/130\n",
    "571/571 [==============================] - 135s 236ms/step - loss: 1.9795\n",
    "\n",
    "Epoch 00028: loss improved from 2.16069 to 2.07170, saving model to bigger_batch.hdf5\n",
    "Epoch 29/130\n",
    "571/571 [==============================] - 135s 236ms/step - loss: 1.9073\n",
    "\n",
    "Epoch 00029: loss improved from 2.07170 to 1.98784, saving model to bigger_batch.hdf5\n",
    "Epoch 30/130\n",
    "571/571 [==============================] - 135s 236ms/step - loss: 1.8375\n",
    "\n",
    "Epoch 00030: loss improved from 1.98784 to 1.90365, saving model to bigger_batch.hdf5\n",
    "Epoch 31/130\n",
    "571/571 [==============================] - 135s 236ms/step - loss: 1.7747\n",
    "\n",
    "Epoch 00031: loss improved from 1.90365 to 1.83011, saving model to bigger_batch.hdf5\n",
    "Epoch 32/130\n",
    "571/571 [==============================] - 135s 236ms/step - loss: 1.6925\n",
    "\n",
    "Epoch 00032: loss improved from 1.83011 to 1.74151, saving model to bigger_batch.hdf5\n",
    "Epoch 33/130\n",
    "571/571 [==============================] - 135s 236ms/step - loss: 1.5950\n",
    "\n",
    "Epoch 00033: loss improved from 1.74151 to 1.67378, saving model to bigger_batch.hdf5\n",
    "Epoch 34/130\n",
    "571/571 [==============================] - 135s 236ms/step - loss: 1.5386\n",
    "\n",
    "Epoch 00034: loss improved from 1.67378 to 1.59937, saving model to bigger_batch.hdf5\n",
    "Epoch 35/130\n",
    "571/571 [==============================] - 135s 236ms/step - loss: 1.4502\n",
    "\n",
    "Epoch 00035: loss improved from 1.59937 to 1.54298, saving model to bigger_batch.hdf5\n",
    "Epoch 36/130\n",
    "571/571 [==============================] - 135s 236ms/step - loss: 1.4021\n",
    "\n",
    "Epoch 00036: loss improved from 1.54298 to 1.48276, saving model to bigger_batch.hdf5\n",
    "Epoch 37/130\n",
    "571/571 [==============================] - 135s 236ms/step - loss: 1.3488\n",
    "\n",
    "Epoch 00037: loss improved from 1.48276 to 1.42724, saving model to bigger_batch.hdf5\n",
    "Epoch 38/130\n",
    "571/571 [==============================] - 135s 236ms/step - loss: 1.2983\n",
    "\n",
    "Epoch 00038: loss improved from 1.42724 to 1.36928, saving model to bigger_batch.hdf5\n",
    "Epoch 39/130\n",
    "571/571 [==============================] - 135s 236ms/step - loss: 1.2224\n",
    "\n",
    "Epoch 00039: loss improved from 1.36928 to 1.30159, saving model to bigger_batch.hdf5\n",
    "Epoch 40/130\n",
    "571/571 [==============================] - 135s 236ms/step - loss: 1.1899\n",
    "\n",
    "Epoch 00040: loss improved from 1.30159 to 1.25716, saving model to bigger_batch.hdf5\n",
    "Epoch 41/130\n",
    "571/571 [==============================] - 135s 236ms/step - loss: 1.1376\n",
    "\n",
    "Epoch 00041: loss improved from 1.25716 to 1.19368, saving model to bigger_batch.hdf5\n",
    "Epoch 42/130\n",
    "571/571 [==============================] - 135s 236ms/step - loss: 1.0802\n",
    "\n",
    "Epoch 00042: loss improved from 1.19368 to 1.14361, saving model to bigger_batch.hdf5\n",
    "Epoch 43/130\n",
    "571/571 [==============================] - 135s 236ms/step - loss: 1.0450\n",
    "\n",
    "Epoch 00043: loss improved from 1.14361 to 1.11238, saving model to bigger_batch.hdf5\n",
    "Epoch 44/130\n",
    "571/571 [==============================] - 135s 236ms/step - loss: 1.0050\n",
    "\n",
    "Epoch 00044: loss improved from 1.11238 to 1.05295, saving model to bigger_batch.hdf5\n",
    "Epoch 45/130\n",
    "571/571 [==============================] - 135s 236ms/step - loss: 0.9346\n",
    "\n",
    "Epoch 00045: loss improved from 1.05295 to 1.00212, saving model to bigger_batch.hdf5\n",
    "Epoch 46/130\n",
    "571/571 [==============================] - 135s 236ms/step - loss: 0.9085\n",
    "\n",
    "Epoch 00046: loss improved from 1.00212 to 0.98160, saving model to bigger_batch.hdf5\n",
    "Epoch 47/130\n",
    "571/571 [==============================] - 136s 238ms/step - loss: 0.8601\n",
    "\n",
    "Epoch 00047: loss improved from 0.98160 to 0.93313, saving model to bigger_batch.hdf5\n",
    "Epoch 48/130\n",
    "571/571 [==============================] - 136s 238ms/step - loss: 0.8481\n",
    "\n",
    "Epoch 00048: loss improved from 0.93313 to 0.90286, saving model to bigger_batch.hdf5\n",
    "Epoch 49/130\n",
    "571/571 [==============================] - 136s 238ms/step - loss: 0.8002\n",
    "\n",
    "Epoch 00049: loss improved from 0.90286 to 0.87300, saving model to bigger_batch.hdf5\n",
    "Epoch 50/130\n",
    "571/571 [==============================] - 136s 238ms/step - loss: 0.7749\n",
    "\n",
    "Epoch 00050: loss improved from 0.87300 to 0.83879, saving model to bigger_batch.hdf5\n",
    "Epoch 51/130\n",
    "571/571 [==============================] - 136s 238ms/step - loss: 0.7491\n",
    "\n",
    "Epoch 00051: loss improved from 0.83879 to 0.80184, saving model to bigger_batch.hdf5\n",
    "Epoch 52/130\n",
    "571/571 [==============================] - 136s 238ms/step - loss: 0.7279\n",
    "\n",
    "Epoch 00052: loss improved from 0.80184 to 0.77472, saving model to bigger_batch.hdf5\n",
    "Epoch 53/130\n",
    "571/571 [==============================] - 136s 238ms/step - loss: 0.6983\n",
    "\n",
    "Epoch 00053: loss improved from 0.77472 to 0.74402, saving model to bigger_batch.hdf5\n",
    "Epoch 54/130\n",
    "571/571 [==============================] - 136s 238ms/step - loss: 0.6830\n",
    "\n",
    "Epoch 00054: loss improved from 0.74402 to 0.72031, saving model to bigger_batch.hdf5\n",
    "Epoch 55/130\n",
    "571/571 [==============================] - 136s 238ms/step - loss: 0.6510\n",
    "\n",
    "Epoch 00055: loss improved from 0.72031 to 0.69218, saving model to bigger_batch.hdf5\n",
    "Epoch 56/130\n",
    "571/571 [==============================] - 136s 238ms/step - loss: 0.6110\n",
    "\n",
    "Epoch 00056: loss improved from 0.69218 to 0.66285, saving model to bigger_batch.hdf5\n",
    "Epoch 57/130\n",
    "571/571 [==============================] - 136s 238ms/step - loss: 0.6057\n",
    "\n",
    "Epoch 00057: loss improved from 0.66285 to 0.65289, saving model to bigger_batch.hdf5\n",
    "Epoch 58/130\n",
    "571/571 [==============================] - 136s 238ms/step - loss: 0.5864\n",
    "\n",
    "Epoch 00058: loss improved from 0.65289 to 0.63242, saving model to bigger_batch.hdf5\n",
    "Epoch 59/130\n",
    "571/571 [==============================] - 136s 238ms/step - loss: 0.5552\n",
    "\n",
    "Epoch 00059: loss improved from 0.63242 to 0.60010, saving model to bigger_batch.hdf5\n",
    "Epoch 60/130\n",
    "571/571 [==============================] - 136s 238ms/step - loss: 0.5490\n",
    "\n",
    "Epoch 00060: loss improved from 0.60010 to 0.59805, saving model to bigger_batch.hdf5\n",
    "Epoch 61/130\n",
    "571/571 [==============================] - 136s 238ms/step - loss: 0.5260\n",
    "\n",
    "Epoch 00061: loss improved from 0.59805 to 0.56780, saving model to bigger_batch.hdf5\n",
    "Epoch 62/130\n",
    "571/571 [==============================] - 136s 238ms/step - loss: 0.5111\n",
    "\n",
    "Epoch 00062: loss improved from 0.56780 to 0.55329, saving model to bigger_batch.hdf5\n",
    "Epoch 63/130\n",
    "571/571 [==============================] - 136s 238ms/step - loss: 0.5037\n",
    "\n",
    "Epoch 00063: loss improved from 0.55329 to 0.53379, saving model to bigger_batch.hdf5\n",
    "Epoch 64/130\n",
    "571/571 [==============================] - 136s 238ms/step - loss: 0.4828\n",
    "\n",
    "Epoch 00064: loss improved from 0.53379 to 0.52112, saving model to bigger_batch.hdf5\n",
    "Epoch 65/130\n",
    "571/571 [==============================] - 136s 238ms/step - loss: 0.4642\n",
    "\n",
    "Epoch 00065: loss improved from 0.52112 to 0.50475, saving model to bigger_batch.hdf5\n",
    "Epoch 66/130\n",
    "571/571 [==============================] - 136s 238ms/step - loss: 0.4419\n",
    "\n",
    "Epoch 00066: loss improved from 0.50475 to 0.47998, saving model to bigger_batch.hdf5\n",
    "Epoch 67/130\n",
    "571/571 [==============================] - 136s 238ms/step - loss: 0.4456\n",
    "\n",
    "Epoch 00067: loss improved from 0.47998 to 0.47790, saving model to bigger_batch.hdf5\n",
    "Epoch 68/130\n",
    "571/571 [==============================] - 136s 238ms/step - loss: 0.4372\n",
    "\n",
    "Epoch 00068: loss improved from 0.47790 to 0.46979, saving model to bigger_batch.hdf5\n",
    "Epoch 69/130\n",
    "571/571 [==============================] - 136s 238ms/step - loss: 0.4137\n",
    "\n",
    "Epoch 00069: loss improved from 0.46979 to 0.44992, saving model to bigger_batch.hdf5\n",
    "Epoch 70/130\n",
    "571/571 [==============================] - 136s 239ms/step - loss: 0.4155\n",
    "\n",
    "Epoch 00070: loss improved from 0.44992 to 0.44644, saving model to bigger_batch.hdf5\n",
    "Epoch 71/130\n",
    "571/571 [==============================] - 136s 239ms/step - loss: 0.4038\n",
    "\n",
    "Epoch 00071: loss improved from 0.44644 to 0.44048, saving model to bigger_batch.hdf5\n",
    "Epoch 72/130\n",
    "571/571 [==============================] - 137s 239ms/step - loss: 0.3999\n",
    "\n",
    "Epoch 00072: loss improved from 0.44048 to 0.42520, saving model to bigger_batch.hdf5\n",
    "Epoch 73/130\n",
    "571/571 [==============================] - 137s 239ms/step - loss: 0.3882\n",
    "\n",
    "Epoch 00073: loss improved from 0.42520 to 0.41858, saving model to bigger_batch.hdf5\n",
    "Epoch 74/130\n",
    "571/571 [==============================] - 136s 239ms/step - loss: 0.3760\n",
    "\n",
    "Epoch 00074: loss improved from 0.41858 to 0.40787, saving model to bigger_batch.hdf5\n",
    "Epoch 75/130\n",
    "571/571 [==============================] - 137s 239ms/step - loss: 0.3760\n",
    "\n",
    "Epoch 00075: loss did not improve from 0.40787\n",
    "Epoch 76/130\n",
    "571/571 [==============================] - 136s 238ms/step - loss: 0.3630\n",
    "\n",
    "Epoch 00076: loss improved from 0.40787 to 0.39094, saving model to bigger_batch.hdf5\n",
    "Epoch 77/130\n",
    "571/571 [==============================] - 136s 238ms/step - loss: 0.3584\n",
    "\n",
    "Epoch 00077: loss improved from 0.39094 to 0.38489, saving model to bigger_batch.hdf5\n",
    "Epoch 78/130\n",
    "571/571 [==============================] - 136s 238ms/step - loss: 0.3467\n",
    "\n",
    "Epoch 00078: loss improved from 0.38489 to 0.37521, saving model to bigger_batch.hdf5\n",
    "Epoch 79/130\n",
    "571/571 [==============================] - 136s 238ms/step - loss: 0.3387\n",
    "\n",
    "Epoch 00079: loss improved from 0.37521 to 0.36526, saving model to bigger_batch.hdf5\n",
    "Epoch 80/130\n",
    "571/571 [==============================] - 136s 238ms/step - loss: 0.3312\n",
    "\n",
    "Epoch 00080: loss improved from 0.36526 to 0.35907, saving model to bigger_batch.hdf5\n",
    "Epoch 81/130\n",
    "571/571 [==============================] - 136s 238ms/step - loss: 0.3329\n",
    "\n",
    "Epoch 00081: loss did not improve from 0.35907\n",
    "Epoch 82/130\n",
    "571/571 [==============================] - 136s 238ms/step - loss: 0.3172\n",
    "\n",
    "Epoch 00082: loss improved from 0.35907 to 0.34742, saving model to bigger_batch.hdf5\n",
    "Epoch 83/130\n",
    "571/571 [==============================] - 136s 238ms/step - loss: 0.3171\n",
    "\n",
    "Epoch 00083: loss improved from 0.34742 to 0.34645, saving model to bigger_batch.hdf5\n",
    "Epoch 84/130\n",
    "571/571 [==============================] - 136s 238ms/step - loss: 0.3020\n",
    "\n",
    "Epoch 00084: loss improved from 0.34645 to 0.33825, saving model to bigger_batch.hdf5\n",
    "Epoch 85/130\n",
    "571/571 [==============================] - 136s 238ms/step - loss: 0.3228\n",
    "\n",
    "Epoch 00085: loss did not improve from 0.33825\n",
    "Epoch 86/130\n",
    "571/571 [==============================] - 136s 238ms/step - loss: 0.3094\n",
    "\n",
    "Epoch 00086: loss improved from 0.33825 to 0.32869, saving model to bigger_batch.hdf5\n",
    "Epoch 87/130\n",
    "571/571 [==============================] - 136s 238ms/step - loss: 0.2934\n",
    "\n",
    "Epoch 00087: loss did not improve from 0.32869\n",
    "Epoch 88/130\n",
    "571/571 [==============================] - 136s 239ms/step - loss: 0.2888\n",
    "\n",
    "Epoch 00088: loss improved from 0.32869 to 0.31903, saving model to bigger_batch.hdf5\n",
    "Epoch 89/130\n",
    "571/571 [==============================] - 136s 239ms/step - loss: 0.3013\n",
    "\n",
    "Epoch 00089: loss did not improve from 0.31903\n",
    "Epoch 90/130\n",
    "571/571 [==============================] - 136s 239ms/step - loss: 0.2946\n",
    "\n",
    "Epoch 00090: loss did not improve from 0.31903\n",
    "Epoch 91/130\n",
    "571/571 [==============================] - 137s 239ms/step - loss: 0.2871\n",
    "\n",
    "Epoch 00091: loss improved from 0.31903 to 0.31323, saving model to bigger_batch.hdf5\n",
    "Epoch 92/130\n",
    "571/571 [==============================] - 137s 240ms/step - loss: 0.2780\n",
    "\n",
    "Epoch 00092: loss improved from 0.31323 to 0.30032, saving model to bigger_batch.hdf5\n",
    "Epoch 93/130\n",
    "571/571 [==============================] - 137s 240ms/step - loss: 0.2817\n",
    "\n",
    "Epoch 00093: loss did not improve from 0.30032\n",
    "Epoch 94/130\n",
    "571/571 [==============================] - 137s 240ms/step - loss: 0.2890\n",
    "\n",
    "Epoch 00094: loss did not improve from 0.30032\n",
    "Epoch 95/130\n",
    "571/571 [==============================] - 137s 240ms/step - loss: 0.2748\n",
    "\n",
    "Epoch 00095: loss improved from 0.30032 to 0.30014, saving model to bigger_batch.hdf5\n",
    "Epoch 96/130\n",
    "571/571 [==============================] - 137s 240ms/step - loss: 0.2729\n",
    "\n",
    "Epoch 00096: loss improved from 0.30014 to 0.29658, saving model to bigger_batch.hdf5\n",
    "Epoch 97/130\n",
    "571/571 [==============================] - 137s 241ms/step - loss: 0.2794\n",
    "\n",
    "Epoch 00097: loss did not improve from 0.29658\n",
    "Epoch 98/130\n",
    "571/571 [==============================] - 137s 240ms/step - loss: 0.2655\n",
    "\n",
    "Epoch 00098: loss improved from 0.29658 to 0.29331, saving model to bigger_batch.hdf5\n",
    "Epoch 99/130\n",
    "571/571 [==============================] - 137s 240ms/step - loss: 0.2720\n",
    "\n",
    "Epoch 00099: loss improved from 0.29331 to 0.29081, saving model to bigger_batch.hdf5\n",
    "Epoch 100/130\n",
    "571/571 [==============================] - 137s 240ms/step - loss: 0.2722\n",
    "\n",
    "Epoch 00100: loss improved from 0.29081 to 0.28979, saving model to bigger_batch.hdf5\n",
    "Epoch 101/130\n",
    "571/571 [==============================] - 137s 240ms/step - loss: 0.2702\n",
    "\n",
    "Epoch 00101: loss improved from 0.28979 to 0.28858, saving model to bigger_batch.hdf5\n",
    "Epoch 102/130\n",
    "571/571 [==============================] - 137s 239ms/step - loss: 0.2640\n",
    "\n",
    "Epoch 00102: loss improved from 0.28858 to 0.28197, saving model to bigger_batch.hdf5\n",
    "Epoch 103/130\n",
    "571/571 [==============================] - 137s 240ms/step - loss: 0.2707\n",
    "\n",
    "Epoch 00103: loss did not improve from 0.28197\n",
    "Epoch 104/130\n",
    "571/571 [==============================] - 137s 240ms/step - loss: 0.2488\n",
    "\n",
    "Epoch 00104: loss improved from 0.28197 to 0.27241, saving model to bigger_batch.hdf5\n",
    "Epoch 105/130\n",
    "571/571 [==============================] - 137s 240ms/step - loss: 0.2605\n",
    "\n",
    "Epoch 00105: loss did not improve from 0.27241\n",
    "Epoch 106/130\n",
    "571/571 [==============================] - 137s 240ms/step - loss: 0.2615\n",
    "\n",
    "Epoch 00106: loss did not improve from 0.27241\n",
    "Epoch 107/130\n",
    "571/571 [==============================] - 137s 240ms/step - loss: 0.2565\n",
    "\n",
    "Epoch 00107: loss did not improve from 0.27241\n",
    "Epoch 108/130\n",
    "571/571 [==============================] - 137s 240ms/step - loss: 0.2537\n",
    "\n",
    "Epoch 00108: loss improved from 0.27241 to 0.27150, saving model to bigger_batch.hdf5\n",
    "Epoch 109/130\n",
    "571/571 [==============================] - 138s 241ms/step - loss: 0.2585\n",
    "\n",
    "Epoch 00109: loss did not improve from 0.27150\n",
    "Epoch 110/130\n",
    "571/571 [==============================] - 138s 241ms/step - loss: 0.2530\n",
    "\n",
    "Epoch 00110: loss did not improve from 0.27150\n",
    "Epoch 111/130\n",
    "571/571 [==============================] - 138s 241ms/step - loss: 0.2498\n",
    "\n",
    "Epoch 00111: loss improved from 0.27150 to 0.27040, saving model to bigger_batch.hdf5\n",
    "Epoch 112/130\n",
    "571/571 [==============================] - 138s 241ms/step - loss: 0.2454\n",
    "\n",
    "Epoch 00112: loss improved from 0.27040 to 0.26650, saving model to bigger_batch.hdf5\n",
    "Epoch 113/130\n",
    "571/571 [==============================] - 137s 241ms/step - loss: 0.2513\n",
    "\n",
    "Epoch 00113: loss did not improve from 0.26650\n",
    "Epoch 114/130\n",
    "571/571 [==============================] - 137s 240ms/step - loss: 0.2464\n",
    "\n",
    "Epoch 00114: loss improved from 0.26650 to 0.26427, saving model to bigger_batch.hdf5\n",
    "Epoch 115/130\n",
    "571/571 [==============================] - 137s 240ms/step - loss: 0.2438\n",
    "\n",
    "Epoch 00115: loss improved from 0.26427 to 0.26126, saving model to bigger_batch.hdf5\n",
    "Epoch 116/130\n",
    "571/571 [==============================] - 137s 240ms/step - loss: 0.2450\n",
    "\n",
    "Epoch 00116: loss did not improve from 0.26126\n",
    "Epoch 117/130\n",
    "571/571 [==============================] - 137s 241ms/step - loss: 0.2533\n",
    "\n",
    "Epoch 00117: loss did not improve from 0.26126\n",
    "Epoch 118/130\n",
    "571/571 [==============================] - 137s 240ms/step - loss: 0.2458\n",
    "\n",
    "Epoch 00118: loss did not improve from 0.26126\n",
    "Epoch 119/130\n",
    "571/571 [==============================] - 137s 240ms/step - loss: 0.2495\n",
    "\n",
    "Epoch 00119: loss did not improve from 0.26126\n",
    "Epoch 120/130\n",
    "571/571 [==============================] - 137s 240ms/step - loss: 0.2535\n",
    "\n",
    "Epoch 00120: loss did not improve from 0.26126\n",
    "Epoch 121/130\n",
    "571/571 [==============================] - 137s 240ms/step - loss: 0.2390\n",
    "\n",
    "Epoch 00121: loss did not improve from 0.26126\n",
    "Epoch 122/130\n",
    "571/571 [==============================] - 137s 240ms/step - loss: 0.2446\n",
    "\n",
    "Epoch 00122: loss improved from 0.26126 to 0.26048, saving model to bigger_batch.hdf5\n",
    "Epoch 123/130\n",
    "571/571 [==============================] - 137s 240ms/step - loss: 0.2461\n",
    "\n",
    "Epoch 00123: loss improved from 0.26048 to 0.26033, saving model to bigger_batch.hdf5\n",
    "Epoch 124/130\n",
    "571/571 [==============================] - 137s 240ms/step - loss: 0.2490\n",
    "\n",
    "Epoch 00124: loss did not improve from 0.26033\n",
    "Epoch 125/130\n",
    "571/571 [==============================] - 137s 240ms/step - loss: 0.2522\n",
    "\n",
    "Epoch 00125: loss improved from 0.26033 to 0.25989, saving model to bigger_batch.hdf5\n",
    "Epoch 126/130\n",
    "571/571 [==============================] - 137s 240ms/step - loss: 0.2508\n",
    "\n",
    "Epoch 00126: loss did not improve from 0.25989\n",
    "Epoch 127/130\n",
    "571/571 [==============================] - 137s 240ms/step - loss: 0.2467\n",
    "\n",
    "Epoch 00127: loss did not improve from 0.25989\n",
    "Epoch 128/130\n",
    "571/571 [==============================] - 137s 240ms/step - loss: 0.2397\n",
    "\n",
    "Epoch 00128: loss did not improve from 0.25989\n",
    "Epoch 129/130\n",
    "571/571 [==============================] - 137s 240ms/step - loss: 0.2495\n",
    "\n",
    "Epoch 00129: loss did not improve from 0.25989\n",
    "Epoch 130/130\n",
    "571/571 [==============================] - 137s 240ms/step - loss: 0.2536'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Attention no time dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_no_time = '''Epoch 1/130 1070/1070 [==============================] - 137s 117ms/step - loss: 6.4991\n",
    "\n",
    "Epoch 00001: loss improved from inf to 6.10377, saving model to best_weights.hdf5 Epoch 2/130 1070/1070 [==============================] - 125s 117ms/step - loss: 5.8330\n",
    "\n",
    "Epoch 00002: loss improved from 6.10377 to 5.83201, saving model to best_weights.hdf5 Epoch 3/130 1070/1070 [==============================] - 125s 117ms/step - loss: 5.7397\n",
    "\n",
    "Epoch 00003: loss improved from 5.83201 to 5.72814, saving model to best_weights.hdf5 Epoch 4/130 1070/1070 [==============================] - 125s 117ms/step - loss: 5.6605\n",
    "\n",
    "Epoch 00004: loss improved from 5.72814 to 5.64703, saving model to best_weights.hdf5 Epoch 5/130 1070/1070 [==============================] - 125s 117ms/step - loss: 5.5885\n",
    "\n",
    "Epoch 00005: loss improved from 5.64703 to 5.58419, saving model to best_weights.hdf5 Epoch 6/130 1070/1070 [==============================] - 125s 116ms/step - loss: 5.5338\n",
    "\n",
    "Epoch 00006: loss improved from 5.58419 to 5.53187, saving model to best_weights.hdf5 Epoch 7/130 1070/1070 [==============================] - 125s 116ms/step - loss: 5.4574\n",
    "\n",
    "Epoch 00007: loss improved from 5.53187 to 5.44906, saving model to best_weights.hdf5 Epoch 8/130 1070/1070 [==============================] - 124s 116ms/step - loss: 5.3628\n",
    "\n",
    "Epoch 00008: loss improved from 5.44906 to 5.34484, saving model to best_weights.hdf5 Epoch 9/130 1070/1070 [==============================] - 125s 117ms/step - loss: 5.2109\n",
    "\n",
    "Epoch 00009: loss improved from 5.34484 to 5.18748, saving model to best_weights.hdf5 Epoch 10/130 1070/1070 [==============================] - 125s 116ms/step - loss: 5.0448\n",
    "\n",
    "Epoch 00010: loss improved from 5.18748 to 5.01242, saving model to best_weights.hdf5 Epoch 11/130 1070/1070 [==============================] - 125s 116ms/step - loss: 4.8626\n",
    "\n",
    "Epoch 00011: loss improved from 5.01242 to 4.86646, saving model to best_weights.hdf5 Epoch 12/130 1070/1070 [==============================] - 125s 116ms/step - loss: 4.7341\n",
    "\n",
    "Epoch 00012: loss improved from 4.86646 to 4.73616, saving model to best_weights.hdf5 Epoch 13/130 1070/1070 [==============================] - 125s 117ms/step - loss: 4.5995\n",
    "\n",
    "Epoch 00013: loss improved from 4.73616 to 4.63512, saving model to best_weights.hdf5 Epoch 14/130 1070/1070 [==============================] - 125s 117ms/step - loss: 4.4920\n",
    "\n",
    "Epoch 00014: loss improved from 4.63512 to 4.51656, saving model to best_weights.hdf5 Epoch 15/130 1070/1070 [==============================] - 125s 117ms/step - loss: 4.3074\n",
    "\n",
    "Epoch 00015: loss improved from 4.51656 to 4.37156, saving model to best_weights.hdf5 Epoch 16/130 1070/1070 [==============================] - 125s 117ms/step - loss: 4.1563\n",
    "\n",
    "Epoch 00016: loss improved from 4.37156 to 4.18215, saving model to best_weights.hdf5 Epoch 17/130 1070/1070 [==============================] - 125s 117ms/step - loss: 3.9231\n",
    "\n",
    "Epoch 00017: loss improved from 4.18215 to 3.98147, saving model to best_weights.hdf5 Epoch 18/130 1070/1070 [==============================] - 125s 117ms/step - loss: 3.6763\n",
    "\n",
    "Epoch 00018: loss improved from 3.98147 to 3.71373, saving model to best_weights.hdf5 Epoch 19/130 1070/1070 [==============================] - 125s 117ms/step - loss: 3.3625\n",
    "\n",
    "Epoch 00019: loss improved from 3.71373 to 3.44110, saving model to best_weights.hdf5 Epoch 20/130 1070/1070 [==============================] - 125s 117ms/step - loss: 3.1015\n",
    "\n",
    "Epoch 00020: loss improved from 3.44110 to 3.19103, saving model to best_weights.hdf5 Epoch 21/130 1070/1070 [==============================] - 125s 117ms/step - loss: 2.8780\n",
    "\n",
    "Epoch 00021: loss improved from 3.19103 to 2.96747, saving model to best_weights.hdf5 Epoch 22/130 1070/1070 [==============================] - 125s 117ms/step - loss: 2.6864\n",
    "\n",
    "Epoch 00022: loss improved from 2.96747 to 2.75873, saving model to best_weights.hdf5 Epoch 23/130 1070/1070 [==============================] - 125s 117ms/step - loss: 2.4693\n",
    "\n",
    "Epoch 00023: loss improved from 2.75873 to 2.54734, saving model to best_weights.hdf5 Epoch 24/130 1070/1070 [==============================] - 125s 117ms/step - loss: 2.2593\n",
    "\n",
    "Epoch 00024: loss improved from 2.54734 to 2.33983, saving model to best_weights.hdf5 Epoch 25/130 1070/1070 [==============================] - 125s 117ms/step - loss: 2.0996\n",
    "\n",
    "Epoch 00025: loss improved from 2.33983 to 2.18121, saving model to best_weights.hdf5 Epoch 26/130 1070/1070 [==============================] - 125s 117ms/step - loss: 1.9222\n",
    "\n",
    "Epoch 00026: loss improved from 2.18121 to 2.02202, saving model to best_weights.hdf5 Epoch 27/130 1070/1070 [==============================] - 128s 120ms/step - loss: 1.7994\n",
    "\n",
    "Epoch 00027: loss improved from 2.02202 to 1.87867, saving model to best_weights.hdf5 Epoch 28/130 1070/1070 [==============================] - 130s 122ms/step - loss: 1.6663\n",
    "\n",
    "Epoch 00028: loss improved from 1.87867 to 1.75159, saving model to best_weights.hdf5 Epoch 29/130 1070/1070 [==============================] - 128s 120ms/step - loss: 1.5500\n",
    "\n",
    "Epoch 00029: loss improved from 1.75159 to 1.64043, saving model to best_weights.hdf5 Epoch 30/130 1070/1070 [==============================] - 125s 117ms/step - loss: 1.4513\n",
    "\n",
    "Epoch 00030: loss improved from 1.64043 to 1.53150, saving model to best_weights.hdf5 Epoch 31/130 1070/1070 [==============================] - 125s 117ms/step - loss: 1.3771\n",
    "\n",
    "Epoch 00031: loss improved from 1.53150 to 1.44433, saving model to best_weights.hdf5 Epoch 32/130 1070/1070 [==============================] - 125s 117ms/step - loss: 1.2622\n",
    "\n",
    "Epoch 00032: loss improved from 1.44433 to 1.34397, saving model to best_weights.hdf5 Epoch 33/130 1070/1070 [==============================] - 125s 117ms/step - loss: 1.2083\n",
    "\n",
    "Epoch 00033: loss improved from 1.34397 to 1.27129, saving model to best_weights.hdf5 Epoch 34/130 1070/1070 [==============================] - 125s 117ms/step - loss: 1.1449\n",
    "\n",
    "Epoch 00034: loss improved from 1.27129 to 1.20377, saving model to best_weights.hdf5 Epoch 35/130 1070/1070 [==============================] - 125s 117ms/step - loss: 1.0824\n",
    "\n",
    "Epoch 00035: loss improved from 1.20377 to 1.14165, saving model to best_weights.hdf5 Epoch 36/130 1070/1070 [==============================] - 125s 117ms/step - loss: 1.0199\n",
    "\n",
    "Epoch 00036: loss improved from 1.14165 to 1.08000, saving model to best_weights.hdf5 Epoch 37/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.9731\n",
    "\n",
    "Epoch 00037: loss improved from 1.08000 to 1.02431, saving model to best_weights.hdf5 Epoch 38/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.9064\n",
    "\n",
    "Epoch 00038: loss improved from 1.02431 to 0.96790, saving model to best_weights.hdf5 Epoch 39/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.8598\n",
    "\n",
    "Epoch 00039: loss improved from 0.96790 to 0.93327, saving model to best_weights.hdf5 Epoch 40/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.8540\n",
    "\n",
    "Epoch 00040: loss improved from 0.93327 to 0.90412, saving model to best_weights.hdf5 Epoch 41/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.8041\n",
    "\n",
    "Epoch 00041: loss improved from 0.90412 to 0.86666, saving model to best_weights.hdf5 Epoch 42/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.7705\n",
    "\n",
    "Epoch 00042: loss improved from 0.86666 to 0.82867, saving model to best_weights.hdf5 Epoch 43/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.7595\n",
    "\n",
    "Epoch 00043: loss improved from 0.82867 to 0.80725, saving model to best_weights.hdf5 Epoch 44/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.7229\n",
    "\n",
    "Epoch 00044: loss improved from 0.80725 to 0.77111, saving model to best_weights.hdf5 Epoch 45/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.7240\n",
    "\n",
    "Epoch 00045: loss improved from 0.77111 to 0.76684, saving model to best_weights.hdf5 Epoch 46/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.6682\n",
    "\n",
    "Epoch 00046: loss improved from 0.76684 to 0.72439, saving model to best_weights.hdf5 Epoch 47/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.6656\n",
    "\n",
    "Epoch 00047: loss improved from 0.72439 to 0.70950, saving model to best_weights.hdf5 Epoch 48/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.6491\n",
    "\n",
    "Epoch 00048: loss improved from 0.70950 to 0.69318, saving model to best_weights.hdf5 Epoch 49/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.6252\n",
    "\n",
    "Epoch 00049: loss improved from 0.69318 to 0.66766, saving model to best_weights.hdf5 Epoch 50/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.6107\n",
    "\n",
    "Epoch 00050: loss improved from 0.66766 to 0.65388, saving model to best_weights.hdf5 Epoch 51/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.5859\n",
    "\n",
    "Epoch 00051: loss improved from 0.65388 to 0.62738, saving model to best_weights.hdf5 Epoch 52/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.5748\n",
    "\n",
    "Epoch 00052: loss improved from 0.62738 to 0.61460, saving model to best_weights.hdf5 Epoch 53/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.5661\n",
    "\n",
    "Epoch 00053: loss improved from 0.61460 to 0.60830, saving model to best_weights.hdf5 Epoch 54/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.5378\n",
    "\n",
    "Epoch 00054: loss improved from 0.60830 to 0.58851, saving model to best_weights.hdf5 Epoch 55/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.5324\n",
    "\n",
    "Epoch 00055: loss improved from 0.58851 to 0.57537, saving model to best_weights.hdf5 Epoch 56/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.5326\n",
    "\n",
    "Epoch 00056: loss improved from 0.57537 to 0.56559, saving model to best_weights.hdf5 Epoch 57/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.5136\n",
    "\n",
    "Epoch 00057: loss improved from 0.56559 to 0.55424, saving model to best_weights.hdf5 Epoch 58/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.5128\n",
    "\n",
    "Epoch 00058: loss improved from 0.55424 to 0.55224, saving model to best_weights.hdf5 Epoch 59/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.4949\n",
    "\n",
    "Epoch 00059: loss improved from 0.55224 to 0.53116, saving model to best_weights.hdf5 Epoch 60/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.4901\n",
    "\n",
    "Epoch 00060: loss improved from 0.53116 to 0.51448, saving model to best_weights.hdf5 Epoch 61/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.4741\n",
    "\n",
    "Epoch 00061: loss improved from 0.51448 to 0.50528, saving model to best_weights.hdf5 Epoch 62/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.4600\n",
    "\n",
    "Epoch 00062: loss improved from 0.50528 to 0.49719, saving model to best_weights.hdf5 Epoch 63/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.4534\n",
    "\n",
    "Epoch 00063: loss improved from 0.49719 to 0.48953, saving model to best_weights.hdf5 Epoch 64/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.4332\n",
    "\n",
    "Epoch 00064: loss improved from 0.48953 to 0.47377, saving model to best_weights.hdf5 Epoch 65/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.4351\n",
    "\n",
    "Epoch 00065: loss improved from 0.47377 to 0.46647, saving model to best_weights.hdf5 Epoch 66/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.4395\n",
    "\n",
    "Epoch 00066: loss did not improve from 0.46647 Epoch 67/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.4329\n",
    "\n",
    "Epoch 00067: loss improved from 0.46647 to 0.46075, saving model to best_weights.hdf5 Epoch 68/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.4185\n",
    "\n",
    "Epoch 00068: loss improved from 0.46075 to 0.45180, saving model to best_weights.hdf5 Epoch 69/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.4171\n",
    "\n",
    "Epoch 00069: loss improved from 0.45180 to 0.44969, saving model to best_weights.hdf5 Epoch 70/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.4084\n",
    "\n",
    "Epoch 00070: loss improved from 0.44969 to 0.43748, saving model to best_weights.hdf5 Epoch 71/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.4016\n",
    "\n",
    "Epoch 00071: loss improved from 0.43748 to 0.43087, saving model to best_weights.hdf5 Epoch 72/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.4111\n",
    "\n",
    "Epoch 00072: loss did not improve from 0.43087 Epoch 73/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.3976\n",
    "\n",
    "Epoch 00073: loss improved from 0.43087 to 0.42148, saving model to best_weights.hdf5 Epoch 74/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.3987\n",
    "\n",
    "Epoch 00074: loss improved from 0.42148 to 0.42103, saving model to best_weights.hdf5 Epoch 75/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.3832\n",
    "\n",
    "Epoch 00075: loss improved from 0.42103 to 0.40714, saving model to best_weights.hdf5 Epoch 76/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.3807\n",
    "\n",
    "Epoch 00076: loss improved from 0.40714 to 0.40010, saving model to best_weights.hdf5 Epoch 77/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.3821\n",
    "\n",
    "Epoch 00077: loss improved from 0.40010 to 0.39894, saving model to best_weights.hdf5 Epoch 78/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.3832\n",
    "\n",
    "Epoch 00078: loss did not improve from 0.39894 Epoch 79/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.3610\n",
    "\n",
    "Epoch 00079: loss improved from 0.39894 to 0.38890, saving model to best_weights.hdf5 Epoch 80/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.3631\n",
    "\n",
    "Epoch 00080: loss improved from 0.38890 to 0.38566, saving model to best_weights.hdf5 Epoch 81/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.3716\n",
    "\n",
    "Epoch 00081: loss did not improve from 0.38566 Epoch 82/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.3526\n",
    "\n",
    "Epoch 00082: loss improved from 0.38566 to 0.38055, saving model to best_weights.hdf5 Epoch 83/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.3520\n",
    "\n",
    "Epoch 00083: loss improved from 0.38055 to 0.37183, saving model to best_weights.hdf5 Epoch 84/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.3466\n",
    "\n",
    "Epoch 00084: loss improved from 0.37183 to 0.37036, saving model to best_weights.hdf5 Epoch 85/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.3591\n",
    "\n",
    "Epoch 00085: loss did not improve from 0.37036 Epoch 86/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.3300\n",
    "\n",
    "Epoch 00086: loss improved from 0.37036 to 0.35270, saving model to best_weights.hdf5 Epoch 87/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.3331\n",
    "\n",
    "Epoch 00087: loss did not improve from 0.35270 Epoch 88/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.3305\n",
    "\n",
    "Epoch 00088: loss improved from 0.35270 to 0.35228, saving model to best_weights.hdf5 Epoch 89/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.3268\n",
    "\n",
    "Epoch 00089: loss improved from 0.35228 to 0.35042, saving model to best_weights.hdf5 Epoch 90/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.3238\n",
    "\n",
    "Epoch 00090: loss improved from 0.35042 to 0.34172, saving model to best_weights.hdf5 Epoch 91/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.3295\n",
    "\n",
    "Epoch 00091: loss did not improve from 0.34172 Epoch 92/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.3202\n",
    "\n",
    "Epoch 00092: loss improved from 0.34172 to 0.33775, saving model to best_weights.hdf5 Epoch 93/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.3139\n",
    "\n",
    "Epoch 00093: loss did not improve from 0.33775 Epoch 94/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.3206\n",
    "\n",
    "Epoch 00094: loss did not improve from 0.33775 Epoch 95/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.3146\n",
    "\n",
    "Epoch 00095: loss improved from 0.33775 to 0.33413, saving model to best_weights.hdf5 Epoch 96/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.3274\n",
    "\n",
    "Epoch 00096: loss did not improve from 0.33413 Epoch 97/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.3020\n",
    "\n",
    "Epoch 00097: loss improved from 0.33413 to 0.33091, saving model to best_weights.hdf5 Epoch 98/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.3115\n",
    "\n",
    "Epoch 00098: loss improved from 0.33091 to 0.32251, saving model to best_weights.hdf5 Epoch 99/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.3136\n",
    "\n",
    "Epoch 00099: loss did not improve from 0.32251 Epoch 100/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.3080\n",
    "\n",
    "Epoch 00100: loss did not improve from 0.32251 Epoch 101/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.2924\n",
    "\n",
    "Epoch 00101: loss improved from 0.32251 to 0.31731, saving model to best_weights.hdf5 Epoch 102/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.2935\n",
    "\n",
    "Epoch 00102: loss improved from 0.31731 to 0.31607, saving model to best_weights.hdf5 Epoch 103/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.3001\n",
    "\n",
    "Epoch 00103: loss did not improve from 0.31607 Epoch 104/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.3111\n",
    "\n",
    "Epoch 00104: loss did not improve from 0.31607 Epoch 105/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.2950\n",
    "\n",
    "Epoch 00105: loss improved from 0.31607 to 0.30830, saving model to best_weights.hdf5 Epoch 106/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.2946\n",
    "\n",
    "Epoch 00106: loss did not improve from 0.30830 Epoch 107/130 1070/1070 [==============================] - 126s 117ms/step - loss: 0.2875\n",
    "\n",
    "Epoch 00107: loss improved from 0.30830 to 0.30667, saving model to best_weights.hdf5 Epoch 108/130 1070/1070 [==============================] - 126s 117ms/step - loss: 0.2835\n",
    "\n",
    "Epoch 00108: loss improved from 0.30667 to 0.30147, saving model to best_weights.hdf5 Epoch 109/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.3008\n",
    "\n",
    "Epoch 00109: loss did not improve from 0.30147 Epoch 110/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.2803\n",
    "\n",
    "Epoch 00110: loss improved from 0.30147 to 0.29817, saving model to best_weights.hdf5 Epoch 111/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.2884\n",
    "\n",
    "Epoch 00111: loss did not improve from 0.29817 Epoch 112/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.2741\n",
    "\n",
    "Epoch 00112: loss improved from 0.29817 to 0.28888, saving model to best_weights.hdf5 Epoch 113/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.2900\n",
    "\n",
    "Epoch 00113: loss did not improve from 0.28888 Epoch 114/130 1070/1070 [==============================] - 126s 117ms/step - loss: 0.2666\n",
    "\n",
    "Epoch 00114: loss improved from 0.28888 to 0.28621, saving model to best_weights.hdf5 Epoch 115/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.2797\n",
    "\n",
    "Epoch 00115: loss did not improve from 0.28621 Epoch 116/130 1070/1070 [==============================] - 126s 117ms/step - loss: 0.2737\n",
    "\n",
    "Epoch 00116: loss did not improve from 0.28621 Epoch 117/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.2753\n",
    "\n",
    "Epoch 00117: loss did not improve from 0.28621 Epoch 118/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.2709\n",
    "\n",
    "Epoch 00118: loss improved from 0.28621 to 0.28462, saving model to best_weights.hdf5 Epoch 119/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.2688\n",
    "\n",
    "Epoch 00119: loss did not improve from 0.28462 Epoch 120/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.2748\n",
    "\n",
    "Epoch 00120: loss did not improve from 0.28462 Epoch 121/130 1070/1070 [==============================] - 126s 117ms/step - loss: 0.2666\n",
    "\n",
    "Epoch 00121: loss improved from 0.28462 to 0.28131, saving model to best_weights.hdf5 Epoch 122/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.2627\n",
    "\n",
    "Epoch 00122: loss did not improve from 0.28131 Epoch 123/130 1070/1070 [==============================] - 126s 117ms/step - loss: 0.2568\n",
    "\n",
    "Epoch 00123: loss improved from 0.28131 to 0.27397, saving model to best_weights.hdf5 Epoch 124/130 1070/1070 [==============================] - 126s 117ms/step - loss: 0.2644\n",
    "\n",
    "Epoch 00124: loss did not improve from 0.27397 Epoch 125/130 1070/1070 [==============================] - 126s 117ms/step - loss: 0.2510\n",
    "\n",
    "Epoch 00125: loss improved from 0.27397 to 0.27286, saving model to best_weights.hdf5 Epoch 126/130 1070/1070 [==============================] - 126s 117ms/step - loss: 0.2638\n",
    "\n",
    "Epoch 00126: loss improved from 0.27286 to 0.27074, saving model to best_weights.hdf5 Epoch 127/130 1070/1070 [==============================] - 126s 117ms/step - loss: 0.2584\n",
    "\n",
    "Epoch 00127: loss did not improve from 0.27074 Epoch 128/130 1070/1070 [==============================] - 126s 117ms/step - loss: 0.2754\n",
    "\n",
    "Epoch 00128: loss did not improve from 0.27074 Epoch 129/130 1070/1070 [==============================] - 126s 117ms/step - loss: 0.2436\n",
    "\n",
    "Epoch 00129: loss improved from 0.27074 to 0.26441, saving model to best_weights.hdf5 Epoch 130/130 1070/1070 [==============================] - 125s 117ms/step - loss: 0.2450'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# below is not attention but with time distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_attention = '''Epoch 1/130\n",
    "1070/1070 [==============================] - 202s 166ms/step - loss: 6.0264\n",
    "\n",
    "Epoch 00001: loss improved from inf to 5.93369, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 2/130\n",
    "1070/1070 [==============================] - 179s 168ms/step - loss: 5.8798\n",
    "\n",
    "Epoch 00002: loss improved from 5.93369 to 5.87253, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 3/130\n",
    "1070/1070 [==============================] - 179s 167ms/step - loss: 5.8542\n",
    "\n",
    "Epoch 00003: loss improved from 5.87253 to 5.85751, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 4/130\n",
    "1070/1070 [==============================] - 179s 167ms/step - loss: 5.8501\n",
    "\n",
    "Epoch 00004: loss improved from 5.85751 to 5.84795, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 5/130\n",
    "1070/1070 [==============================] - 179s 167ms/step - loss: 5.8392\n",
    "\n",
    "Epoch 00005: loss improved from 5.84795 to 5.83626, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 6/130\n",
    "1070/1070 [==============================] - 179s 167ms/step - loss: 5.8294\n",
    "\n",
    "Epoch 00006: loss improved from 5.83626 to 5.82882, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 7/130\n",
    "1070/1070 [==============================] - 179s 168ms/step - loss: 5.7988\n",
    "\n",
    "Epoch 00007: loss improved from 5.82882 to 5.77573, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 8/130\n",
    "1070/1070 [==============================] - 180s 168ms/step - loss: 5.6920\n",
    "\n",
    "Epoch 00008: loss improved from 5.77573 to 5.67272, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 9/130\n",
    "1070/1070 [==============================] - 181s 169ms/step - loss: 5.6423\n",
    "\n",
    "Epoch 00009: loss improved from 5.67272 to 5.65456, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 10/130\n",
    "1070/1070 [==============================] - 180s 169ms/step - loss: 5.6121\n",
    "\n",
    "Epoch 00010: loss improved from 5.65456 to 5.62886, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 11/130\n",
    "1070/1070 [==============================] - 179s 168ms/step - loss: 5.6214\n",
    "\n",
    "Epoch 00011: loss improved from 5.62886 to 5.61719, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 12/130\n",
    "1070/1070 [==============================] - 180s 168ms/step - loss: 5.5794\n",
    "\n",
    "Epoch 00012: loss improved from 5.61719 to 5.59620, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 13/130\n",
    "1070/1070 [==============================] - 180s 169ms/step - loss: 5.5398\n",
    "\n",
    "Epoch 00013: loss improved from 5.59620 to 5.55076, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 14/130\n",
    "1070/1070 [==============================] - 180s 168ms/step - loss: 5.4714\n",
    "\n",
    "Epoch 00014: loss improved from 5.55076 to 5.45799, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 15/130\n",
    "1070/1070 [==============================] - 180s 168ms/step - loss: 5.4096\n",
    "\n",
    "Epoch 00015: loss improved from 5.45799 to 5.43679, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 16/130\n",
    "1070/1070 [==============================] - 180s 168ms/step - loss: 5.3577\n",
    "\n",
    "Epoch 00016: loss improved from 5.43679 to 5.33982, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 17/130\n",
    "1070/1070 [==============================] - 179s 167ms/step - loss: 5.2648\n",
    "\n",
    "Epoch 00017: loss improved from 5.33982 to 5.27134, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 18/130\n",
    "1070/1070 [==============================] - 179s 167ms/step - loss: 5.1915\n",
    "\n",
    "Epoch 00018: loss improved from 5.27134 to 5.19777, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 19/130\n",
    "1070/1070 [==============================] - 180s 168ms/step - loss: 5.1247\n",
    "\n",
    "Epoch 00019: loss improved from 5.19777 to 5.15271, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 20/130\n",
    "1070/1070 [==============================] - 179s 167ms/step - loss: 5.0395\n",
    "\n",
    "Epoch 00020: loss improved from 5.15271 to 5.02475, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 21/130\n",
    "1070/1070 [==============================] - 179s 167ms/step - loss: 4.9364\n",
    "\n",
    "Epoch 00021: loss improved from 5.02475 to 4.98161, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 22/130\n",
    "1070/1070 [==============================] - 179s 167ms/step - loss: 4.9269\n",
    "\n",
    "Epoch 00022: loss improved from 4.98161 to 4.92584, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 23/130\n",
    "1070/1070 [==============================] - 179s 167ms/step - loss: 4.7872\n",
    "\n",
    "Epoch 00023: loss improved from 4.92584 to 4.81128, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 24/130\n",
    "1070/1070 [==============================] - 179s 167ms/step - loss: 4.6763\n",
    "\n",
    "Epoch 00024: loss improved from 4.81128 to 4.73945, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 25/130\n",
    "1070/1070 [==============================] - 180s 168ms/step - loss: 4.7045\n",
    "\n",
    "Epoch 00025: loss improved from 4.73945 to 4.68172, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 26/130\n",
    "1070/1070 [==============================] - 180s 168ms/step - loss: 4.5554\n",
    "\n",
    "Epoch 00026: loss improved from 4.68172 to 4.59586, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 27/130\n",
    "1070/1070 [==============================] - 179s 168ms/step - loss: 4.4973\n",
    "\n",
    "Epoch 00027: loss improved from 4.59586 to 4.53395, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 28/130\n",
    "1070/1070 [==============================] - 178s 167ms/step - loss: 4.4197\n",
    "\n",
    "Epoch 00028: loss improved from 4.53395 to 4.44966, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 29/130\n",
    "1070/1070 [==============================] - 178s 167ms/step - loss: 4.3198\n",
    "\n",
    "Epoch 00029: loss improved from 4.44966 to 4.36873, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 30/130\n",
    "1070/1070 [==============================] - 180s 168ms/step - loss: 4.2713\n",
    "\n",
    "Epoch 00030: loss improved from 4.36873 to 4.31663, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 31/130\n",
    "1070/1070 [==============================] - 180s 168ms/step - loss: 4.1201\n",
    "\n",
    "Epoch 00031: loss improved from 4.31663 to 4.15165, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 32/130\n",
    "1070/1070 [==============================] - 179s 168ms/step - loss: 4.0576\n",
    "\n",
    "Epoch 00032: loss improved from 4.15165 to 4.11172, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 33/130\n",
    "1070/1070 [==============================] - 179s 168ms/step - loss: 3.9801\n",
    "\n",
    "Epoch 00033: loss improved from 4.11172 to 4.01649, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 34/130\n",
    "1070/1070 [==============================] - 179s 168ms/step - loss: 3.8492\n",
    "\n",
    "Epoch 00034: loss improved from 4.01649 to 3.90037, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 35/130\n",
    "1070/1070 [==============================] - 179s 167ms/step - loss: 3.7656\n",
    "\n",
    "Epoch 00035: loss improved from 3.90037 to 3.78684, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 36/130\n",
    "1070/1070 [==============================] - 179s 168ms/step - loss: 3.6463\n",
    "\n",
    "Epoch 00036: loss improved from 3.78684 to 3.70988, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 37/130\n",
    "1070/1070 [==============================] - 179s 167ms/step - loss: 3.5258\n",
    "\n",
    "Epoch 00037: loss improved from 3.70988 to 3.62394, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 38/130\n",
    "1070/1070 [==============================] - 179s 167ms/step - loss: 3.6030\n",
    "\n",
    "Epoch 00038: loss did not improve from 3.62394\n",
    "Epoch 39/130\n",
    "1070/1070 [==============================] - 180s 168ms/step - loss: 3.4260\n",
    "\n",
    "Epoch 00039: loss improved from 3.62394 to 3.46786, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 40/130\n",
    "1070/1070 [==============================] - 181s 169ms/step - loss: 3.3240\n",
    "\n",
    "Epoch 00040: loss improved from 3.46786 to 3.36994, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 41/130\n",
    "1070/1070 [==============================] - 181s 169ms/step - loss: 3.2485\n",
    "\n",
    "Epoch 00041: loss improved from 3.36994 to 3.29758, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 42/130\n",
    "1070/1070 [==============================] - 181s 169ms/step - loss: 3.1789\n",
    "\n",
    "Epoch 00042: loss improved from 3.29758 to 3.21585, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 43/130\n",
    "1070/1070 [==============================] - 180s 168ms/step - loss: 3.0146\n",
    "\n",
    "Epoch 00043: loss improved from 3.21585 to 3.09567, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 44/130\n",
    "1070/1070 [==============================] - 180s 169ms/step - loss: 2.9847\n",
    "\n",
    "Epoch 00044: loss improved from 3.09567 to 3.02942, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 45/130\n",
    "1070/1070 [==============================] - 180s 169ms/step - loss: 2.9383\n",
    "\n",
    "Epoch 00045: loss improved from 3.02942 to 2.99925, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 46/130\n",
    "1070/1070 [==============================] - 180s 168ms/step - loss: 2.9116\n",
    "\n",
    "Epoch 00046: loss improved from 2.99925 to 2.95276, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 47/130\n",
    "1070/1070 [==============================] - 180s 168ms/step - loss: 2.7844\n",
    "\n",
    "Epoch 00047: loss improved from 2.95276 to 2.83542, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 48/130\n",
    "1070/1070 [==============================] - 181s 169ms/step - loss: 2.7333\n",
    "\n",
    "Epoch 00048: loss improved from 2.83542 to 2.76822, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 49/130\n",
    "1070/1070 [==============================] - 180s 168ms/step - loss: 2.6040\n",
    "\n",
    "Epoch 00049: loss improved from 2.76822 to 2.67064, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 50/130\n",
    "1070/1070 [==============================] - 179s 167ms/step - loss: 2.5619\n",
    "\n",
    "Epoch 00050: loss improved from 2.67064 to 2.59512, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 51/130\n",
    "1070/1070 [==============================] - 180s 168ms/step - loss: 2.4636\n",
    "\n",
    "Epoch 00051: loss improved from 2.59512 to 2.49875, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 52/130\n",
    "1070/1070 [==============================] - 181s 169ms/step - loss: 2.3673\n",
    "\n",
    "Epoch 00052: loss improved from 2.49875 to 2.44055, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 53/130\n",
    "1070/1070 [==============================] - 180s 169ms/step - loss: 2.2957\n",
    "\n",
    "Epoch 00053: loss improved from 2.44055 to 2.35355, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 54/130\n",
    "1070/1070 [==============================] - 181s 169ms/step - loss: 2.2517\n",
    "\n",
    "Epoch 00054: loss improved from 2.35355 to 2.30179, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 55/130\n",
    "1070/1070 [==============================] - 181s 169ms/step - loss: 2.1982\n",
    "\n",
    "Epoch 00055: loss improved from 2.30179 to 2.25487, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 56/130\n",
    "1070/1070 [==============================] - 180s 168ms/step - loss: 2.1245\n",
    "\n",
    "Epoch 00056: loss improved from 2.25487 to 2.18442, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 57/130\n",
    "1070/1070 [==============================] - 179s 167ms/step - loss: 2.0103\n",
    "\n",
    "Epoch 00057: loss improved from 2.18442 to 2.08381, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 58/130\n",
    "1070/1070 [==============================] - 179s 168ms/step - loss: 1.9748\n",
    "\n",
    "Epoch 00058: loss improved from 2.08381 to 2.02941, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 59/130\n",
    "1070/1070 [==============================] - 180s 168ms/step - loss: 1.8828\n",
    "\n",
    "Epoch 00059: loss improved from 2.02941 to 1.94466, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 60/130\n",
    "1070/1070 [==============================] - 180s 168ms/step - loss: 1.8673\n",
    "\n",
    "Epoch 00060: loss improved from 1.94466 to 1.91019, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 61/130\n",
    "1070/1070 [==============================] - 180s 168ms/step - loss: 1.8024\n",
    "\n",
    "Epoch 00061: loss improved from 1.91019 to 1.85357, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 62/130\n",
    "1070/1070 [==============================] - 180s 168ms/step - loss: 1.7836\n",
    "\n",
    "Epoch 00062: loss improved from 1.85357 to 1.83402, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 63/130\n",
    "1070/1070 [==============================] - 182s 170ms/step - loss: 1.7322\n",
    "\n",
    "Epoch 00063: loss improved from 1.83402 to 1.78764, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 64/130\n",
    "1070/1070 [==============================] - 182s 170ms/step - loss: 1.6938\n",
    "\n",
    "Epoch 00064: loss improved from 1.78764 to 1.75110, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 65/130\n",
    "1070/1070 [==============================] - 182s 170ms/step - loss: 1.6669\n",
    "\n",
    "Epoch 00065: loss improved from 1.75110 to 1.71031, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 66/130\n",
    "1070/1070 [==============================] - 181s 169ms/step - loss: 1.6490\n",
    "\n",
    "Epoch 00066: loss improved from 1.71031 to 1.68874, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 67/130\n",
    "1070/1070 [==============================] - 181s 169ms/step - loss: 1.5840\n",
    "\n",
    "Epoch 00067: loss improved from 1.68874 to 1.63741, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 68/130\n",
    "1070/1070 [==============================] - 180s 169ms/step - loss: 1.5551\n",
    "\n",
    "Epoch 00068: loss improved from 1.63741 to 1.60225, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 69/130\n",
    "1070/1070 [==============================] - 181s 169ms/step - loss: 1.4905\n",
    "\n",
    "Epoch 00069: loss improved from 1.60225 to 1.56343, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 70/130\n",
    "1070/1070 [==============================] - 181s 169ms/step - loss: 1.4673\n",
    "\n",
    "Epoch 00070: loss improved from 1.56343 to 1.53692, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 71/130\n",
    "1070/1070 [==============================] - 181s 169ms/step - loss: 1.4492\n",
    "\n",
    "Epoch 00071: loss improved from 1.53692 to 1.49739, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 72/130\n",
    "1070/1070 [==============================] - 181s 170ms/step - loss: 1.3892\n",
    "\n",
    "Epoch 00072: loss improved from 1.49739 to 1.44710, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 73/130\n",
    "1070/1070 [==============================] - 182s 170ms/step - loss: 1.3430\n",
    "\n",
    "Epoch 00073: loss improved from 1.44710 to 1.40669, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 74/130\n",
    "1070/1070 [==============================] - 182s 170ms/step - loss: 1.3154\n",
    "\n",
    "Epoch 00074: loss improved from 1.40669 to 1.38591, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 75/130\n",
    "1070/1070 [==============================] - 182s 170ms/step - loss: 1.3180\n",
    "\n",
    "Epoch 00075: loss improved from 1.38591 to 1.36628, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 76/130\n",
    "1070/1070 [==============================] - 181s 169ms/step - loss: 1.2954\n",
    "\n",
    "Epoch 00076: loss improved from 1.36628 to 1.35006, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 77/130\n",
    "1070/1070 [==============================] - 180s 168ms/step - loss: 1.2705\n",
    "\n",
    "Epoch 00077: loss improved from 1.35006 to 1.31740, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 78/130\n",
    "1070/1070 [==============================] - 180s 169ms/step - loss: 1.2538\n",
    "\n",
    "Epoch 00078: loss improved from 1.31740 to 1.30021, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 79/130\n",
    "1070/1070 [==============================] - 181s 169ms/step - loss: 1.2281\n",
    "\n",
    "Epoch 00079: loss improved from 1.30021 to 1.28200, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 80/130\n",
    "1070/1070 [==============================] - 181s 169ms/step - loss: 1.2286\n",
    "\n",
    "Epoch 00080: loss improved from 1.28200 to 1.27203, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 81/130\n",
    "1070/1070 [==============================] - 180s 168ms/step - loss: 1.1819\n",
    "\n",
    "Epoch 00081: loss improved from 1.27203 to 1.23585, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 82/130\n",
    "1070/1070 [==============================] - 180s 168ms/step - loss: 1.1550\n",
    "\n",
    "Epoch 00082: loss improved from 1.23585 to 1.20742, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 83/130\n",
    "1070/1070 [==============================] - 181s 169ms/step - loss: 1.1600\n",
    "\n",
    "Epoch 00083: loss improved from 1.20742 to 1.19993, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 84/130\n",
    "1070/1070 [==============================] - 182s 170ms/step - loss: 1.1344\n",
    "\n",
    "Epoch 00084: loss improved from 1.19993 to 1.18212, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 85/130\n",
    "1070/1070 [==============================] - 182s 170ms/step - loss: 1.1184\n",
    "\n",
    "Epoch 00085: loss improved from 1.18212 to 1.16001, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 86/130\n",
    "1070/1070 [==============================] - 182s 170ms/step - loss: 1.0727\n",
    "\n",
    "Epoch 00086: loss improved from 1.16001 to 1.11052, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 87/130\n",
    "1070/1070 [==============================] - 182s 170ms/step - loss: 1.0631\n",
    "\n",
    "Epoch 00087: loss did not improve from 1.11052\n",
    "Epoch 88/130\n",
    "1070/1070 [==============================] - 182s 170ms/step - loss: 1.0271\n",
    "\n",
    "Epoch 00088: loss improved from 1.11052 to 1.06968, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 89/130\n",
    "1070/1070 [==============================] - 181s 169ms/step - loss: 1.0268\n",
    "\n",
    "Epoch 00089: loss did not improve from 1.06968\n",
    "Epoch 90/130\n",
    "1070/1070 [==============================] - 181s 169ms/step - loss: 1.0117\n",
    "\n",
    "Epoch 00090: loss improved from 1.06968 to 1.05210, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 91/130\n",
    "1070/1070 [==============================] - 181s 169ms/step - loss: 0.9721\n",
    "\n",
    "Epoch 00091: loss improved from 1.05210 to 1.03370, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 92/130\n",
    "1070/1070 [==============================] - 181s 169ms/step - loss: 0.9749\n",
    "\n",
    "Epoch 00092: loss improved from 1.03370 to 1.02831, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 93/130\n",
    "1070/1070 [==============================] - 181s 169ms/step - loss: 0.9741\n",
    "\n",
    "Epoch 00093: loss improved from 1.02831 to 1.01033, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 94/130\n",
    "1070/1070 [==============================] - 181s 169ms/step - loss: 0.9516\n",
    "\n",
    "Epoch 00094: loss improved from 1.01033 to 1.00212, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 95/130\n",
    "1070/1070 [==============================] - 181s 169ms/step - loss: 0.9566\n",
    "\n",
    "Epoch 00095: loss improved from 1.00212 to 0.98542, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 96/130\n",
    "1070/1070 [==============================] - 182s 170ms/step - loss: 0.9356\n",
    "\n",
    "Epoch 00096: loss improved from 0.98542 to 0.97327, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 97/130\n",
    "1070/1070 [==============================] - 180s 168ms/step - loss: 0.9296\n",
    "\n",
    "Epoch 00097: loss did not improve from 0.97327\n",
    "Epoch 98/130\n",
    "1070/1070 [==============================] - 181s 169ms/step - loss: 0.9106\n",
    "\n",
    "Epoch 00098: loss improved from 0.97327 to 0.95874, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 99/130\n",
    "1070/1070 [==============================] - 181s 169ms/step - loss: 0.9077\n",
    "\n",
    "Epoch 00099: loss improved from 0.95874 to 0.94502, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 100/130\n",
    "1070/1070 [==============================] - 181s 169ms/step - loss: 0.9010\n",
    "\n",
    "Epoch 00100: loss improved from 0.94502 to 0.94035, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 101/130\n",
    "1070/1070 [==============================] - 181s 169ms/step - loss: 0.8969\n",
    "\n",
    "Epoch 00101: loss improved from 0.94035 to 0.94018, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 102/130\n",
    "1070/1070 [==============================] - 181s 169ms/step - loss: 0.9064\n",
    "\n",
    "Epoch 00102: loss improved from 0.94018 to 0.93230, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 103/130\n",
    "1070/1070 [==============================] - 181s 170ms/step - loss: 0.8531\n",
    "\n",
    "Epoch 00103: loss improved from 0.93230 to 0.89530, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 104/130\n",
    "1070/1070 [==============================] - 181s 169ms/step - loss: 0.8304\n",
    "\n",
    "Epoch 00104: loss did not improve from 0.89530\n",
    "Epoch 105/130\n",
    "1070/1070 [==============================] - 181s 170ms/step - loss: 0.8420\n",
    "\n",
    "Epoch 00105: loss improved from 0.89530 to 0.87876, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 106/130\n",
    "1070/1070 [==============================] - 182s 170ms/step - loss: 0.8350\n",
    "\n",
    "Epoch 00106: loss did not improve from 0.87876\n",
    "Epoch 107/130\n",
    "1070/1070 [==============================] - 182s 170ms/step - loss: 0.8487\n",
    "\n",
    "Epoch 00107: loss did not improve from 0.87876\n",
    "Epoch 108/130\n",
    "1070/1070 [==============================] - 182s 170ms/step - loss: 0.8263\n",
    "\n",
    "Epoch 00108: loss improved from 0.87876 to 0.87240, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 109/130\n",
    "1070/1070 [==============================] - 182s 170ms/step - loss: 0.8257\n",
    "\n",
    "Epoch 00109: loss improved from 0.87240 to 0.86731, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 110/130\n",
    "1070/1070 [==============================] - 181s 170ms/step - loss: 0.8103\n",
    "\n",
    "Epoch 00110: loss improved from 0.86731 to 0.85853, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 111/130\n",
    "1070/1070 [==============================] - 181s 169ms/step - loss: 0.8085\n",
    "\n",
    "Epoch 00111: loss improved from 0.85853 to 0.84178, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 112/130\n",
    "1070/1070 [==============================] - 181s 169ms/step - loss: 0.8073\n",
    "\n",
    "Epoch 00112: loss did not improve from 0.84178\n",
    "Epoch 113/130\n",
    "1070/1070 [==============================] - 181s 169ms/step - loss: 0.7771\n",
    "\n",
    "Epoch 00113: loss improved from 0.84178 to 0.83198, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 114/130\n",
    "1070/1070 [==============================] - 181s 169ms/step - loss: 0.8086\n",
    "\n",
    "Epoch 00114: loss did not improve from 0.83198\n",
    "Epoch 115/130\n",
    "1070/1070 [==============================] - 180s 168ms/step - loss: 0.7940\n",
    "\n",
    "Epoch 00115: loss improved from 0.83198 to 0.82852, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 116/130\n",
    "1070/1070 [==============================] - 179s 167ms/step - loss: 0.7918\n",
    "\n",
    "Epoch 00116: loss improved from 0.82852 to 0.82488, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 117/130\n",
    "1070/1070 [==============================] - 179s 167ms/step - loss: 0.7791\n",
    "\n",
    "Epoch 00117: loss improved from 0.82488 to 0.81707, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 118/130\n",
    "1070/1070 [==============================] - 181s 169ms/step - loss: 0.7839\n",
    "\n",
    "Epoch 00118: loss did not improve from 0.81707\n",
    "Epoch 119/130\n",
    "1070/1070 [==============================] - 182s 170ms/step - loss: 0.7693\n",
    "\n",
    "Epoch 00119: loss improved from 0.81707 to 0.81205, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 120/130\n",
    "1070/1070 [==============================] - 182s 170ms/step - loss: 0.7822\n",
    "\n",
    "Epoch 00120: loss did not improve from 0.81205\n",
    "Epoch 121/130\n",
    "1070/1070 [==============================] - 181s 169ms/step - loss: 0.7627\n",
    "\n",
    "Epoch 00121: loss improved from 0.81205 to 0.80657, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 122/130\n",
    "1070/1070 [==============================] - 181s 169ms/step - loss: 0.7790\n",
    "\n",
    "Epoch 00122: loss did not improve from 0.80657\n",
    "Epoch 123/130\n",
    "1070/1070 [==============================] - 181s 169ms/step - loss: 0.7591\n",
    "\n",
    "Epoch 00123: loss improved from 0.80657 to 0.80655, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 124/130\n",
    "1070/1070 [==============================] - 182s 170ms/step - loss: 0.7882\n",
    "\n",
    "Epoch 00124: loss did not improve from 0.80655\n",
    "Epoch 125/130\n",
    "1070/1070 [==============================] - 182s 170ms/step - loss: 0.7561\n",
    "\n",
    "Epoch 00125: loss improved from 0.80655 to 0.79880, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 126/130\n",
    "1070/1070 [==============================] - 182s 170ms/step - loss: 0.7458\n",
    "\n",
    "Epoch 00126: loss improved from 0.79880 to 0.79269, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 127/130\n",
    "1070/1070 [==============================] - 181s 169ms/step - loss: 0.7490\n",
    "\n",
    "Epoch 00127: loss improved from 0.79269 to 0.78714, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 128/130\n",
    "1070/1070 [==============================] - 181s 169ms/step - loss: 0.7448\n",
    "\n",
    "Epoch 00128: loss improved from 0.78714 to 0.77859, saving model to weights_no_attentnion.hdf5\n",
    "Epoch 129/130\n",
    "1070/1070 [==============================] - 181s 170ms/step - loss: 0.7575\n",
    "\n",
    "Epoch 00129: loss did not improve from 0.77859\n",
    "Epoch 130/130\n",
    "1070/1070 [==============================] - 181s 169ms/step - loss: 0.7371\n",
    "\n",
    "Epoch 00130: loss improved from 0.77859 to 0.77745'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch variable\n",
    "no_attention # --- > With time dist layer but no attention layer\n",
    "attention_no_time #----> with attention layer but no time dist \n",
    "bb_no_att_time_dist # --- > larger batch size no attention with time dist\n",
    "bb_att_time # -----  > With attention and time\n",
    "bb_att_no_time # ---- > Big batch with attention and no time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loss(x):\n",
    "    x = x.split(' ')\n",
    "    string_list = []\n",
    "    for i, value in enumerate(x):\n",
    "        if value == 'loss:':\n",
    "            string_list.append(i +1)\n",
    "            \n",
    "    real_list = []\n",
    "    for value in string_list:\n",
    "        real_list.append(x[value])\n",
    "        \n",
    "    no_att_time_dist =[]\n",
    "    for loss in real_list:\n",
    "        no_att_time_dist.append(float(loss[:6]))\n",
    "        \n",
    "    return no_att_time_dist\n",
    "bb_att_no_time = create_loss(bb_att_no_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = create_loss(bb_no_att_time_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "space = np.linspace(0,130, 130)\n",
    "fig, ax = plt.subplots(figsize =(8,5))\n",
    "ax.plot(space, no_attention_with_time, label='Time No Attention', linewidth=3, alpha =.8)\n",
    "ax.plot(space, attention_no_time, label='Attention No Time', linewidth=3, alpha =.8)\n",
    "ax.plot(space, bb_no_att_time_dist, label='Big Batch Time Only', linewidth=3, alpha =.8)\n",
    "ax.plot(space, bb_att_time, label='Big Batch Attention + Time', linewidth=3, alpha =.8)\n",
    "ax.plot(space, bb_att_no_time, label='Big Batch Attention No Time', linewidth=3, alpha =.8)\n",
    "\n",
    "ax.yaxis.grid(color='black', linestyle='dashed',alpha =.4)\n",
    "ax.xaxis.grid(color='black', linestyle='dashed', alpha =.4)\n",
    "\n",
    "ax.set_title('Models Learning Over Time', fontsize=22,weight = 'bold' )\n",
    "\n",
    "plt.xlabel(\"Epochs\", fontsize=18, weight = 'bold')\n",
    "plt.ylabel(\"Error\", fontsize=18, weight = 'bold')\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.patch.set_alpha(0.1)\n",
    "fig.set_facecolor('b')\n",
    "\n",
    "\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "plt.xticks( fontsize = 12, weight = 'bold')\n",
    "plt.yticks(fontsize = 12, weight = 'bold')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.savefig('images/models_learning.jpg', format='jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(attention_no_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(no_attention_with_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
